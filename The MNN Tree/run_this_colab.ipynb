{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyM3ORyjGXKSOod5BNgfCPhW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#Content\n","1.   Library (Required)\n","2.   EMNIST Dataset\n","> 2.1 Run the Training \\\n","> 2.2 Run the Inference \\\n","> 2.3 Demo \\\n","1.   SVHN Dataset\n","> 3.1 Run the Training \\\n","> 3.2 Run the Inference \\\n","> 3.3 Demo \\\n","2.   Other Dataset\n","> 4.1 Run the Training \\\n","> 4.2 Run the Inference \\\n","> 4.3 Demo \\\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"lzLp2qJgQkVo"}},{"cell_type":"markdown","source":["# 1. Library (Required)\n","Run this code parts is compulsory"],"metadata":{"id":"lpAJWpTFQAdB"}},{"cell_type":"code","source":["import numpy as np, numpy.random\n","\n","np.random.dirichlet(np.ones(10)*1000.,size=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zidpGXwdkIjw","executionInfo":{"status":"ok","timestamp":1685341110178,"user_tz":-480,"elapsed":3,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"2da88bb5-9034-417f-91a7-afda933d0746"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.09617608, 0.1043546 , 0.09668223, 0.10111669, 0.09948076,\n","        0.09738855, 0.10481861, 0.10217982, 0.09979067, 0.09801197]])"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["#delete these row if will not use COLAB\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G4iPTy1PlRb8","executionInfo":{"status":"ok","timestamp":1684384661735,"user_tz":-480,"elapsed":14724,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"d2d001fa-133b-429c-aecb-c86eb65830fc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["!pip install thop"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gv38XwH-DPUC","executionInfo":{"status":"ok","timestamp":1684384667860,"user_tz":-480,"elapsed":4745,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"9f3aa62e-7503-49a2-8951-71d5002ad2f7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting thop\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.0.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (16.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n","Installing collected packages: thop\n","Successfully installed thop-0.1.1.post2209072238\n"]}]},{"cell_type":"markdown","source":["# 2. EMNIST Dataset"],"metadata":{"id":"2_FKZ-POEPYj"}},{"cell_type":"markdown","source":["## 2.0 Grouper"],"metadata":{"id":"sHpDX7k8I2p9"}},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/SoftmaxOutput/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jurRo7ruI7GU","executionInfo":{"status":"ok","timestamp":1684384679547,"user_tz":-480,"elapsed":1320,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"83ad8b25-4227-4743-da2d-696b6b69eea0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/SoftmaxOutput\n"]}]},{"cell_type":"code","source":["!python grouper.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7xuYTi6JDaU","executionInfo":{"status":"ok","timestamp":1684384695656,"user_tz":-480,"elapsed":14420,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"b763b477-e7c7-4cf2-e280-a1818985432b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["root:\n","[0, 13, 24, 36, 2, 35, 26, 44, 8, 11, 41, 9]\n","[1, 18, 21, 19]\n","[3]\n","[4, 17, 30, 34, 10, 31, 33, 20, 27, 42, 37, 6, 16]\n","[5, 28]\n","[7]\n","[12, 39]\n","[14]\n","[15, 40, 46, 29]\n","[22, 43]\n","[23, 32]\n","[25]\n","[38]\n","[45]\n","\n","\n","0:\n","[0, 24]\n","[13]\n","[36]\n","[2, 35]\n","[26]\n","[44, 41, 9, 8]\n","[11]\n","\n","\n","1:\n","[1, 18, 21]\n","[19]\n","\n","\n","3:\n","[4, 34]\n","[17]\n","[30]\n","[10]\n","[31]\n","[33]\n","[20]\n","[27]\n","[42]\n","[37]\n","[6]\n","[16]\n","\n","\n","8:\n","[15, 40]\n","[46]\n","[29]\n"]}]},{"cell_type":"markdown","source":["## 2.1 Run the Training"],"metadata":{"id":"rxUk3_gkEkIZ"}},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Train/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"puG9z-ukG_vx","executionInfo":{"status":"ok","timestamp":1684382718653,"user_tz":-480,"elapsed":1161,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"001f353a-ee47-469e-a3d7-c22aacd21a9d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Train\n"]}]},{"cell_type":"code","source":["!python emnist.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sUDL5Y1YHEAJ","executionInfo":{"status":"ok","timestamp":1672036649151,"user_tz":-480,"elapsed":580642,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"f534bbab-6b72-476f-e5b4-87fab13ff39b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to dataEMNIST/EMNIST/raw/gzip.zip\n","100% 561753746/561753746 [00:26<00:00, 21354012.42it/s]\n","Extracting dataEMNIST/EMNIST/raw/gzip.zip to dataEMNIST/EMNIST/raw\n"," 1126/1126 [================================================================================>]  Step: 78ms | Tot: 23s52ms | Loss: 0.0004 | Acc: 80.378% (90486/112575)\n","Epoch:  1 \n","Train:  0.8037841439040639\n","Val:  0.8488888888888889\n","checkpoint saved\n"," 1126/1126 [================================================================================>]  Step: 66ms | Tot: 25s12ms | Loss: 0.0003 | Acc: 86.492% (97368/112575)\n","Epoch:  2 \n","Train:  0.8649167221852099\n","Val:  0.8488888888888889\n"," 1126/1126 [================================================================================>]  Step: 62ms | Tot: 22s982ms | Loss: 0.0002 | Acc: 87.760% (98796/112575)\n","Epoch:  3 \n","Train:  0.877601598934044\n","Val:  0.8577777777777778\n","checkpoint saved\n"," 1126/1126 [================================================================================>]  Step: 64ms | Tot: 23s20ms | Loss: 0.0003 | Acc: 88.275% (99376/112575)\n","Epoch:  4 \n","Train:  0.8827537197423939\n","Val:  0.8622222222222222\n","checkpoint saved\n"," 1126/1126 [================================================================================>]  Step: 65ms | Tot: 22s990ms | Loss: 0.0004 | Acc: 88.838% (100009/112575)\n","Epoch:  5 \n","Train:  0.8883766377970242\n","Val:  0.8755555555555555\n","checkpoint saved\n"," 1126/1126 [================================================================================>]  Step: 56ms | Tot: 22s982ms | Loss: 0.0002 | Acc: 89.154% (100365/112575)\n","Epoch:  6 \n","Train:  0.8915389740173217\n","Val:  0.8711111111111111\n"," 1126/1126 [================================================================================>]  Step: 53ms | Tot: 22s976ms | Loss: 0.0002 | Acc: 89.619% (100889/112575)\n","Epoch:  7 \n","Train:  0.8961936486786587\n","Val:  0.8711111111111111\n"," 1126/1126 [================================================================================>]  Step: 54ms | Tot: 22s916ms | Loss: 0.0001 | Acc: 89.873% (101175/112575)\n","Epoch:  8 \n","Train:  0.8987341772151899\n","Val:  0.8888888888888888\n","checkpoint saved\n"," 1126/1126 [================================================================================>]  Step: 54ms | Tot: 23s64ms | Loss: 0.0002 | Acc: 90.202% (101545/112575)\n","Epoch:  9 \n","Train:  0.9020208749722407\n","Val:  0.88\n"," 1126/1126 [================================================================================>]  Step: 51ms | Tot: 22s843ms | Loss: 0.0001 | Acc: 90.490% (101869/112575)\n","Epoch:  10 \n","Train:  0.9048989562513879\n","Val:  0.8755555555555555\n"," 1126/1126 [================================================================================>]  Step: 56ms | Tot: 23s45ms | Loss: 0.0003 | Acc: 90.569% (101958/112575)\n","Epoch:  11 \n","Train:  0.9056895403064623\n","Val:  0.8711111111111111\n"," 1126/1126 [================================================================================>]  Step: 125ms | Tot: 27s310ms | Loss: 0.0003 | Acc: 90.905% (102336/112575)\n","Epoch:  12 \n","Train:  0.9090473017988008\n","Val:  0.8533333333333334\n"," 1126/1126 [================================================================================>]  Step: 53ms | Tot: 24s106ms | Loss: 0.0002 | Acc: 91.168% (102632/112575)\n","Epoch:  13 \n","Train:  0.9116766600044415\n","Val:  0.8533333333333334\n"," 1126/1126 [================================================================================>]  Step: 63ms | Tot: 26s256ms | Loss: 0.0001 | Acc: 91.346% (102833/112575)\n","Epoch:  14 \n","Train:  0.9134621363535421\n","Val:  0.8666666666666667\n"," 1126/1126 [================================================================================>]  Step: 54ms | Tot: 28s543ms | Loss: 0.0002 | Acc: 91.386% (102878/112575)\n","Epoch:  15 \n","Train:  0.9138618698645348\n","Val:  0.8622222222222222\n"," 1126/1126 [================================================================================>]  Step: 52ms | Tot: 22s840ms | Loss: 0.0002 | Acc: 91.633% (103156/112575)\n","Epoch:  16 \n","Train:  0.9163313346657783\n","Val:  0.8755555555555555\n"," 1126/1126 [================================================================================>]  Step: 53ms | Tot: 23s327ms | Loss: 0.0001 | Acc: 91.853% (103403/112575)\n","Epoch:  17 \n","Train:  0.9185254274927825\n","Val:  0.8577777777777778\n"," 1126/1126 [================================================================================>]  Step: 51ms | Tot: 22s795ms | Loss: 0.0002 | Acc: 91.954% (103517/112575)\n","Epoch:  18 \n","Train:  0.9195380857206307\n","Val:  0.8755555555555555\n"," 1126/1126 [================================================================================>]  Step: 50ms | Tot: 23s486ms | Loss: 0.0002 | Acc: 92.171% (103762/112575)\n","Epoch:  19 \n","Train:  0.921714412613813\n","Val:  0.8533333333333334\n"," 1126/1126 [================================================================================>]  Step: 50ms | Tot: 22s726ms | Loss: 0.0002 | Acc: 92.305% (103912/112575)\n","Epoch:  20 \n","Train:  0.9230468576504552\n","Val:  0.8622222222222222\n","emnist.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  outputs = m(outputs)\n","tensor([[7.2178e-01, 6.3924e-06, 5.9047e-05, 3.5024e-05, 4.6896e-06, 5.4131e-05,\n","         1.6146e-04, 3.6020e-06, 1.6183e-04, 4.7201e-05, 1.6133e-05, 2.1765e-04,\n","         5.6011e-04, 1.0227e-02, 8.6747e-07, 1.2581e-05, 1.3566e-04, 3.1301e-07,\n","         2.6018e-05, 3.2685e-04, 3.6488e-08, 2.8457e-05, 2.0895e-07, 4.2748e-05,\n","         2.6035e-01, 6.7538e-05, 3.8165e-04, 8.9768e-07, 3.8670e-05, 1.2640e-06,\n","         1.7471e-03, 1.5576e-04, 3.0740e-06, 6.5435e-07, 1.6898e-05, 1.0843e-05,\n","         8.3945e-04, 4.9229e-04, 4.9326e-04, 7.6947e-05, 3.9696e-06, 6.5006e-04,\n","         3.1194e-06, 2.6582e-04, 1.3427e-04, 6.0330e-05, 2.9763e-04]],\n","       device='cuda:0')\n","tensor([[5.5218e-05, 5.1587e-01, 2.2509e-04, 2.3318e-05, 1.0594e-04, 2.5198e-05,\n","         1.2805e-04, 1.0200e-03, 2.0400e-05, 2.4565e-05, 4.1968e-05, 1.3277e-06,\n","         4.1567e-04, 2.8761e-05, 5.0730e-06, 1.0621e-04, 1.7820e-06, 1.2070e-05,\n","         1.7602e-01, 2.3964e-03, 2.1581e-05, 2.9773e-01, 2.7130e-06, 6.8493e-05,\n","         3.9195e-05, 1.0504e-04, 5.0383e-07, 4.0937e-07, 3.9040e-05, 3.7301e-04,\n","         8.2095e-05, 5.4818e-04, 1.4404e-05, 4.3871e-06, 6.7975e-04, 3.2736e-05,\n","         1.3710e-05, 3.7345e-04, 1.5834e-04, 3.3251e-05, 3.2325e-04, 1.2387e-04,\n","         3.0828e-04, 2.7748e-04, 1.2502e-04, 1.0108e-03, 9.8214e-04]],\n","       device='cuda:0')\n","tensor([[3.8685e-06, 7.3891e-05, 9.5039e-01, 1.8211e-04, 3.0593e-06, 2.7268e-06,\n","         4.1540e-06, 4.2345e-04, 4.1397e-05, 1.0900e-06, 2.0927e-06, 3.7528e-06,\n","         3.0143e-05, 4.7042e-05, 8.0115e-07, 1.0001e-05, 1.2006e-06, 3.6358e-07,\n","         1.8232e-04, 4.0482e-04, 2.8443e-06, 1.6152e-03, 5.4992e-08, 8.9474e-05,\n","         1.9901e-06, 8.5531e-08, 8.1543e-05, 4.2108e-05, 8.8749e-05, 1.3251e-05,\n","         2.1215e-05, 9.1458e-04, 1.7354e-06, 7.0575e-05, 5.8795e-05, 4.2965e-02,\n","         1.1382e-03, 4.5703e-05, 8.5807e-05, 3.8915e-05, 2.2628e-05, 6.7286e-05,\n","         5.3609e-05, 1.4034e-04, 3.9699e-04, 8.9901e-05, 1.4598e-04]],\n","       device='cuda:0')\n","tensor([[2.9503e-06, 2.2373e-06, 2.8595e-04, 9.9709e-01, 1.7569e-09, 1.0509e-04,\n","         1.1201e-06, 2.9552e-05, 5.6998e-05, 7.6504e-06, 2.4346e-09, 4.0554e-04,\n","         1.9994e-08, 6.5024e-06, 4.0797e-09, 2.3987e-06, 8.9028e-06, 3.9546e-10,\n","         3.5060e-06, 8.7003e-04, 4.8069e-10, 2.3863e-06, 1.6305e-10, 1.2051e-07,\n","         4.6969e-06, 1.2065e-05, 1.5142e-07, 1.6296e-08, 9.3107e-05, 1.8737e-07,\n","         1.2137e-07, 1.3763e-08, 1.3973e-08, 1.2334e-07, 1.1900e-05, 9.0045e-04,\n","         2.5422e-06, 2.0316e-05, 4.1692e-07, 3.5604e-08, 2.6246e-06, 6.9827e-05,\n","         2.6005e-08, 7.3628e-08, 3.3593e-06, 1.1870e-07, 4.3221e-07]],\n","       device='cuda:0')\n","tensor([[1.4398e-05, 9.1173e-06, 8.0573e-06, 1.0743e-07, 9.6079e-01, 7.1410e-06,\n","         1.6643e-04, 3.9059e-06, 3.9019e-05, 4.9813e-04, 3.1816e-04, 5.1437e-07,\n","         3.5663e-05, 4.7976e-07, 6.5566e-08, 6.5191e-05, 9.1623e-05, 1.6696e-03,\n","         5.7387e-05, 1.0730e-04, 1.5519e-05, 6.9759e-05, 3.0821e-05, 3.9475e-05,\n","         3.7891e-06, 7.0982e-07, 2.9397e-05, 3.0107e-07, 4.3998e-05, 1.5823e-05,\n","         2.2606e-03, 7.0442e-05, 6.7534e-05, 2.0983e-04, 2.9842e-02, 3.9575e-06,\n","         6.3549e-04, 1.3345e-05, 1.7746e-04, 6.1446e-06, 3.2073e-04, 1.6408e-04,\n","         3.0238e-04, 1.6311e-04, 9.0809e-04, 1.0775e-05, 7.1607e-04]],\n","       device='cuda:0')\n","tensor([[2.7765e-05, 2.6560e-07, 3.7727e-05, 7.6031e-04, 1.8260e-07, 9.5252e-01,\n","         2.5210e-04, 1.2356e-05, 4.5861e-04, 2.4744e-04, 1.8237e-08, 3.9011e-05,\n","         2.2055e-04, 9.8485e-06, 8.6191e-06, 1.0429e-04, 3.2934e-04, 1.4678e-08,\n","         7.1517e-05, 1.9738e-03, 3.0477e-07, 3.6430e-06, 4.8715e-10, 1.3377e-07,\n","         5.6343e-05, 2.5396e-08, 2.7828e-06, 1.6313e-08, 4.2077e-02, 1.6364e-05,\n","         1.1544e-05, 6.8582e-06, 2.3663e-07, 4.9796e-07, 3.3883e-05, 2.9691e-06,\n","         8.0249e-07, 6.8015e-05, 6.4100e-07, 3.2408e-06, 7.2266e-05, 2.5619e-04,\n","         3.1340e-06, 2.2301e-07, 4.3902e-05, 2.3611e-04, 2.8338e-05]],\n","       device='cuda:0')\n","tensor([[5.9899e-05, 4.3125e-05, 3.0363e-05, 5.5810e-06, 1.4755e-05, 3.5509e-04,\n","         9.4557e-01, 2.3708e-08, 1.6381e-04, 2.6470e-07, 3.9248e-08, 5.7247e-05,\n","         1.5331e-03, 2.5592e-06, 1.6046e-05, 1.1778e-05, 1.2849e-02, 4.9172e-07,\n","         1.5391e-04, 2.6851e-05, 3.8347e-05, 1.5633e-03, 1.9539e-08, 2.7505e-07,\n","         2.4825e-05, 4.4630e-07, 2.7259e-05, 1.2153e-06, 9.9031e-04, 7.7802e-07,\n","         3.9460e-04, 7.4579e-06, 2.0669e-05, 3.2996e-05, 5.9123e-07, 1.1217e-05,\n","         4.9915e-05, 3.4988e-02, 1.1058e-05, 1.3142e-05, 8.2408e-05, 1.4972e-04,\n","         5.1920e-04, 8.0499e-07, 4.8116e-05, 6.4785e-05, 6.8429e-05]],\n","       device='cuda:0')\n","tensor([[1.3988e-05, 3.8427e-04, 3.5780e-04, 6.7446e-05, 6.3207e-05, 3.2248e-06,\n","         2.2016e-09, 9.9525e-01, 1.8428e-06, 2.8781e-04, 3.7233e-06, 5.9032e-07,\n","         1.5035e-07, 5.2966e-05, 2.6883e-09, 4.4590e-05, 1.2699e-07, 8.3391e-07,\n","         1.3614e-04, 1.9579e-04, 8.6062e-09, 1.8682e-05, 6.7041e-05, 2.0841e-06,\n","         1.5498e-05, 2.3857e-05, 3.5648e-07, 9.7827e-08, 2.0954e-07, 6.8889e-04,\n","         2.2178e-07, 1.8258e-06, 1.3466e-06, 9.4690e-07, 7.7409e-04, 3.4346e-04,\n","         2.1313e-05, 1.4225e-07, 3.9512e-06, 4.1643e-07, 7.1721e-05, 3.8458e-05,\n","         7.4327e-08, 6.8033e-04, 3.4992e-05, 4.9984e-05, 2.9403e-04]],\n","       device='cuda:0')\n","tensor([[1.1692e-04, 1.8932e-06, 2.8839e-04, 1.1872e-04, 2.5704e-05, 3.3970e-04,\n","         1.0137e-04, 3.1673e-07, 9.8774e-01, 5.7156e-04, 5.7524e-06, 1.6961e-03,\n","         5.7408e-05, 7.0387e-05, 2.0517e-05, 1.5757e-04, 1.5253e-04, 2.3782e-07,\n","         6.9232e-06, 5.8952e-05, 2.2414e-05, 2.5422e-04, 2.6917e-10, 4.2308e-06,\n","         2.3384e-04, 3.1861e-04, 8.4488e-05, 5.7615e-06, 2.9617e-04, 1.3554e-06,\n","         7.0987e-06, 2.9546e-05, 4.3325e-07, 1.5263e-04, 1.4629e-04, 1.0346e-04,\n","         3.6307e-05, 1.3084e-04, 9.2775e-05, 8.7204e-05, 4.0488e-04, 2.5198e-03,\n","         1.7247e-06, 1.3208e-06, 3.3439e-03, 1.3042e-04, 5.9329e-05]],\n","       device='cuda:0')\n","tensor([[9.0677e-05, 1.2781e-05, 2.4171e-05, 1.0929e-04, 1.1568e-03, 3.5413e-05,\n","         5.0759e-07, 9.6209e-04, 4.6682e-04, 8.1316e-01, 1.2129e-05, 5.5454e-06,\n","         3.2141e-07, 1.5941e-05, 4.0784e-06, 9.2849e-05, 2.1694e-04, 2.9107e-06,\n","         1.9737e-05, 1.0616e-05, 1.3429e-08, 6.5266e-05, 1.8319e-06, 4.7388e-06,\n","         3.6560e-04, 1.3393e-04, 6.5627e-04, 1.3968e-06, 3.0041e-04, 8.1145e-05,\n","         5.6167e-05, 1.1301e-06, 1.1113e-06, 9.8886e-07, 1.1796e-03, 1.3230e-05,\n","         1.5436e-03, 1.8692e-06, 1.6462e-06, 3.7546e-05, 2.1562e-04, 4.6578e-02,\n","         1.2811e-05, 5.3114e-05, 1.3225e-01, 1.0940e-05, 2.4106e-05]],\n","       device='cuda:0')\n","tensor([[2.8907e-05, 2.3814e-04, 2.2621e-04, 1.1338e-05, 1.3073e-03, 1.6985e-07,\n","         1.0354e-06, 1.5392e-05, 7.3324e-06, 1.9079e-04, 9.7193e-01, 2.9118e-04,\n","         6.3889e-06, 1.1742e-04, 1.3996e-07, 5.0230e-04, 7.7331e-05, 3.3247e-03,\n","         4.6590e-05, 3.3711e-05, 3.7454e-04, 2.0259e-04, 2.3217e-04, 3.9173e-04,\n","         1.0056e-04, 8.0072e-04, 1.7426e-04, 6.3905e-04, 1.3244e-06, 3.5959e-05,\n","         8.1666e-06, 6.6981e-07, 2.9346e-05, 3.3037e-04, 1.2766e-04, 9.2055e-06,\n","         1.1448e-02, 7.1751e-05, 2.5381e-04, 5.7289e-05, 1.0031e-03, 4.1952e-04,\n","         1.0597e-03, 1.6127e-03, 3.5203e-04, 1.1108e-03, 7.9459e-04]],\n","       device='cuda:0')\n","tensor([[1.1235e-04, 6.3665e-08, 6.3939e-05, 2.3400e-03, 2.7826e-08, 1.9889e-05,\n","         1.4623e-05, 1.8731e-06, 1.1503e-03, 3.8392e-05, 4.3620e-05, 9.9203e-01,\n","         1.1525e-05, 3.7136e-04, 4.1116e-06, 9.2188e-06, 3.8855e-04, 7.2312e-07,\n","         4.4629e-07, 1.8181e-05, 2.6948e-05, 6.2518e-06, 2.1246e-05, 1.4879e-05,\n","         1.7664e-04, 8.3118e-05, 4.5094e-05, 8.6575e-05, 3.8725e-05, 1.5927e-06,\n","         2.0693e-06, 3.6417e-07, 1.1521e-06, 2.3730e-05, 4.1705e-06, 9.2227e-04,\n","         5.8083e-05, 1.1587e-03, 1.1991e-05, 5.2092e-05, 1.1738e-05, 1.0109e-04,\n","         4.9444e-05, 1.2065e-06, 5.3011e-05, 4.2385e-04, 2.3392e-06]],\n","       device='cuda:0')\n","tensor([[2.5281e-04, 2.1095e-05, 2.2293e-05, 3.4644e-07, 2.0614e-06, 5.8210e-05,\n","         5.2958e-04, 5.2375e-08, 2.4678e-06, 6.8073e-07, 4.1356e-08, 8.9260e-07,\n","         9.9180e-01, 1.5671e-07, 7.9908e-05, 3.1736e-05, 2.0299e-04, 6.4712e-08,\n","         3.4473e-04, 1.5811e-05, 4.0565e-06, 1.2907e-03, 3.0438e-08, 1.6068e-05,\n","         4.1040e-04, 3.9229e-07, 1.3973e-05, 2.6583e-06, 5.6482e-05, 4.7030e-06,\n","         9.1274e-04, 2.0767e-04, 1.9223e-06, 3.3290e-05, 2.1290e-06, 3.9866e-05,\n","         1.1962e-04, 3.9651e-06, 2.0743e-05, 2.2116e-03, 5.1683e-05, 2.9241e-05,\n","         3.0948e-06, 2.4254e-05, 8.4142e-06, 8.8135e-04, 2.8627e-04]],\n","       device='cuda:0')\n","tensor([[2.2854e-02, 5.3046e-06, 2.8026e-04, 1.3231e-04, 1.3275e-07, 8.2238e-05,\n","         4.4861e-06, 1.2937e-04, 6.7515e-05, 2.9061e-05, 3.1581e-05, 6.4068e-04,\n","         1.3597e-05, 9.5623e-01, 1.9466e-06, 4.4063e-05, 8.7006e-06, 1.8576e-07,\n","         5.6083e-05, 1.2566e-03, 2.4548e-08, 2.5686e-05, 2.6147e-06, 5.1390e-04,\n","         1.2186e-02, 1.7039e-03, 1.0203e-04, 1.4993e-05, 1.0250e-04, 9.0053e-06,\n","         5.3510e-04, 1.4280e-04, 1.5417e-05, 1.9206e-06, 1.1937e-05, 1.9432e-05,\n","         3.2624e-05, 1.2218e-03, 7.6909e-04, 1.5787e-05, 1.3683e-05, 8.7534e-05,\n","         1.8948e-05, 3.9149e-04, 5.4717e-05, 7.4888e-05, 6.6016e-05]],\n","       device='cuda:0')\n","tensor([[8.4851e-06, 1.4646e-06, 2.0607e-04, 4.2425e-07, 9.1388e-06, 5.6964e-04,\n","         1.2735e-04, 3.2874e-06, 2.7754e-04, 5.3509e-05, 4.3096e-06, 5.2175e-05,\n","         1.1180e-03, 6.1676e-08, 9.8880e-01, 1.3721e-03, 3.7893e-04, 1.3149e-06,\n","         1.0246e-04, 1.0985e-05, 1.9334e-04, 2.7461e-05, 8.8534e-08, 8.6472e-08,\n","         2.7179e-05, 3.2355e-09, 4.6996e-07, 6.9324e-05, 1.9997e-04, 3.1962e-07,\n","         1.8284e-05, 6.0220e-07, 3.2452e-07, 1.7646e-05, 2.3990e-06, 6.2315e-04,\n","         8.1265e-05, 4.9912e-06, 1.4498e-05, 2.7941e-03, 1.1581e-03, 1.7019e-04,\n","         2.1949e-05, 9.8244e-07, 4.6700e-04, 6.2090e-05, 9.4981e-04]],\n","       device='cuda:0')\n","tensor([[1.9522e-04, 2.7019e-04, 1.2781e-04, 2.4889e-04, 8.3256e-05, 7.0024e-04,\n","         2.1845e-04, 4.7385e-04, 3.7164e-04, 9.4137e-05, 8.0103e-05, 3.3406e-05,\n","         2.6860e-04, 4.4044e-04, 3.9229e-04, 6.3710e-01, 3.9356e-04, 2.5943e-05,\n","         4.3342e-04, 7.4443e-04, 1.5912e-04, 2.7622e-04, 1.1380e-05, 5.2775e-06,\n","         5.8654e-04, 8.5424e-04, 3.6813e-06, 3.1647e-05, 6.9463e-04, 6.5536e-04,\n","         9.5181e-07, 1.7430e-05, 1.7013e-05, 3.9707e-04, 5.1466e-04, 5.3112e-04,\n","         8.1740e-05, 7.8614e-06, 1.0952e-05, 6.5965e-04, 3.4644e-01, 2.7026e-04,\n","         8.1451e-05, 1.9258e-05, 2.7897e-04, 1.2147e-03, 3.4804e-03]],\n","       device='cuda:0')\n","tensor([[5.0641e-04, 4.8838e-06, 5.0416e-05, 4.4765e-05, 3.4028e-05, 3.5453e-04,\n","         9.9193e-03, 1.6154e-09, 6.0883e-05, 3.3145e-04, 3.8681e-06, 4.7897e-04,\n","         1.7714e-03, 1.7564e-05, 2.3358e-04, 2.6000e-04, 9.7584e-01, 1.0677e-05,\n","         6.9634e-05, 1.2946e-04, 3.8945e-06, 3.5495e-05, 8.7737e-07, 2.6118e-05,\n","         2.2069e-04, 2.4482e-07, 7.7473e-04, 3.4009e-07, 1.2020e-03, 4.8032e-08,\n","         4.7002e-04, 7.4153e-07, 2.7817e-06, 7.1109e-07, 2.2844e-05, 1.8005e-05,\n","         6.5881e-04, 7.2811e-04, 4.3718e-05, 6.6220e-05, 5.5754e-04, 4.6049e-03,\n","         8.8357e-05, 1.1351e-06, 3.0793e-04, 3.8419e-06, 3.6653e-05]],\n","       device='cuda:0')\n","tensor([[2.0412e-08, 6.8436e-06, 8.7571e-06, 1.4781e-08, 2.8755e-03, 4.4941e-06,\n","         1.2060e-06, 4.4389e-07, 8.2414e-07, 9.7047e-06, 2.3318e-04, 1.9184e-05,\n","         2.8247e-05, 1.4094e-06, 1.6431e-07, 3.6417e-05, 6.0863e-05, 9.9043e-01,\n","         9.9817e-05, 1.1849e-04, 4.1392e-04, 3.8593e-04, 5.0727e-04, 1.0769e-03,\n","         8.5278e-07, 4.8566e-06, 3.7930e-07, 1.6290e-05, 1.0111e-05, 1.1158e-05,\n","         2.0018e-04, 4.6860e-06, 1.9029e-04, 2.3224e-04, 7.6563e-04, 1.3792e-05,\n","         7.2307e-05, 1.9528e-06, 1.3301e-05, 1.7653e-06, 2.0208e-04, 2.1046e-05,\n","         1.1644e-03, 3.4525e-04, 3.8157e-06, 1.7842e-04, 2.2940e-04]],\n","       device='cuda:0')\n","tensor([[9.0547e-05, 1.7903e-01, 4.6883e-04, 1.3997e-04, 7.5830e-05, 8.0502e-05,\n","         4.1339e-04, 1.2218e-04, 5.0509e-05, 1.5727e-05, 1.0002e-05, 4.6262e-06,\n","         1.7192e-03, 1.7780e-04, 1.9502e-05, 5.5412e-04, 6.2278e-06, 5.2091e-06,\n","         5.5444e-01, 2.6876e-02, 5.0386e-05, 2.2440e-01, 1.2170e-06, 1.2897e-04,\n","         1.3539e-04, 1.3597e-04, 1.0860e-06, 3.7848e-06, 1.5307e-04, 9.9120e-04,\n","         9.3356e-04, 6.9753e-04, 1.7683e-05, 6.4925e-05, 4.9700e-04, 2.7885e-04,\n","         1.0844e-04, 9.4955e-04, 1.3608e-04, 4.9747e-04, 6.6944e-04, 1.5964e-04,\n","         2.8338e-04, 3.5264e-04, 1.2828e-04, 2.0456e-03, 1.8783e-03]],\n","       device='cuda:0')\n","tensor([[3.4504e-05, 6.3445e-04, 2.7503e-04, 4.8670e-04, 2.6899e-05, 1.5076e-03,\n","         1.2012e-05, 1.0118e-03, 2.5247e-04, 5.1805e-06, 7.5683e-06, 1.9783e-06,\n","         2.4106e-05, 2.8316e-04, 1.7994e-06, 8.9737e-05, 1.4696e-05, 1.3493e-05,\n","         6.7812e-03, 9.7941e-01, 5.9676e-06, 1.0531e-03, 1.7268e-07, 6.5347e-05,\n","         4.5549e-05, 4.6717e-06, 7.5639e-06, 7.3729e-08, 2.0535e-03, 4.5945e-04,\n","         1.8317e-03, 9.0844e-04, 4.3518e-05, 7.3438e-05, 6.6441e-04, 1.9540e-04,\n","         3.2323e-05, 7.5674e-05, 1.1084e-04, 2.5458e-06, 4.8511e-04, 2.6791e-04,\n","         3.2699e-05, 1.6577e-05, 3.9471e-05, 5.6347e-05, 5.9045e-04]],\n","       device='cuda:0')\n","tensor([[2.8787e-08, 1.7613e-04, 3.1287e-05, 1.2833e-06, 3.0556e-05, 2.1667e-05,\n","         2.8257e-05, 1.7498e-07, 1.5961e-06, 3.8637e-08, 2.7488e-05, 8.5176e-06,\n","         5.9890e-06, 1.6517e-08, 2.2717e-05, 4.3123e-05, 1.4826e-05, 6.2705e-04,\n","         1.2865e-04, 3.7498e-05, 9.8927e-01, 6.3265e-04, 1.2258e-05, 1.4166e-04,\n","         4.3097e-08, 4.2914e-06, 1.5242e-07, 4.6787e-04, 1.8361e-06, 2.3733e-05,\n","         2.9797e-05, 3.4824e-05, 2.7317e-05, 2.9451e-03, 1.4372e-04, 2.7556e-05,\n","         3.3052e-06, 1.1832e-04, 1.2809e-06, 4.0024e-05, 5.2480e-04, 1.0021e-06,\n","         1.7876e-03, 1.9113e-05, 8.1452e-06, 1.2878e-03, 1.2379e-03]],\n","       device='cuda:0')\n","tensor([[4.0564e-05, 2.3181e-01, 1.2469e-03, 2.8765e-05, 1.3883e-04, 3.9513e-05,\n","         2.3377e-03, 9.9371e-05, 3.4831e-05, 1.9788e-05, 8.5691e-06, 6.3194e-06,\n","         5.9232e-03, 2.6554e-05, 2.9910e-05, 1.3919e-04, 2.2219e-05, 6.8821e-06,\n","         1.6329e-01, 4.3727e-03, 1.4918e-04, 5.7859e-01, 7.5703e-07, 5.5014e-05,\n","         2.5100e-05, 8.2952e-05, 1.4707e-05, 7.8130e-06, 4.8933e-04, 3.9085e-04,\n","         3.8473e-04, 4.0734e-04, 2.1095e-05, 1.1234e-04, 8.4555e-04, 1.0970e-04,\n","         2.6384e-04, 1.1158e-03, 1.6637e-04, 6.5271e-04, 5.4600e-04, 1.0607e-04,\n","         3.3166e-03, 1.4936e-04, 9.6181e-05, 7.7930e-04, 1.4937e-03]],\n","       device='cuda:0')\n","tensor([[3.6513e-07, 2.1536e-07, 7.5247e-07, 1.1981e-07, 4.7274e-06, 2.6094e-08,\n","         1.1500e-09, 2.6172e-05, 1.3053e-09, 4.6022e-07, 1.3647e-05, 6.6652e-09,\n","         3.2973e-07, 2.7823e-06, 4.9269e-11, 2.9632e-07, 1.0393e-06, 2.5692e-04,\n","         1.7164e-07, 2.2396e-07, 9.8633e-06, 2.7454e-07, 9.9571e-01, 7.7485e-04,\n","         3.1448e-06, 3.5057e-07, 1.0223e-08, 1.0620e-06, 1.6024e-08, 4.1369e-07,\n","         3.0295e-05, 9.2896e-07, 2.9474e-05, 1.5532e-06, 5.2904e-05, 3.1469e-06,\n","         1.0730e-05, 2.2389e-07, 1.7344e-06, 3.3879e-07, 8.5873e-06, 6.8809e-07,\n","         1.4459e-04, 2.8470e-03, 2.7789e-07, 5.8178e-05, 4.5901e-07]],\n","       device='cuda:0')\n","tensor([[1.4093e-05, 1.0507e-06, 1.0921e-04, 9.9859e-09, 6.8922e-06, 6.1837e-09,\n","         6.0735e-06, 9.6467e-06, 9.0045e-08, 2.6381e-06, 5.8331e-05, 2.0479e-06,\n","         3.4731e-05, 8.2972e-05, 2.6582e-07, 1.0862e-06, 8.8645e-07, 1.1338e-03,\n","         6.2887e-05, 4.9041e-04, 6.8344e-05, 3.6383e-06, 2.6203e-04, 9.6677e-01,\n","         3.9287e-05, 3.9426e-05, 3.3609e-05, 1.4535e-05, 1.0363e-04, 3.2566e-07,\n","         3.0163e-04, 1.3363e-03, 2.0133e-03, 1.3443e-04, 3.2781e-05, 3.0399e-06,\n","         6.5368e-04, 1.8463e-05, 2.1903e-04, 4.0376e-04, 7.7726e-06, 3.7488e-05,\n","         6.9539e-05, 2.4824e-02, 1.7952e-05, 5.7491e-04, 3.6156e-06]],\n","       device='cuda:0')\n","tensor([[3.4461e-01, 5.7445e-06, 8.2881e-05, 2.8483e-05, 1.8143e-06, 4.0236e-05,\n","         8.9639e-05, 1.0673e-06, 2.0718e-04, 2.8633e-05, 1.6532e-05, 2.5527e-04,\n","         7.5346e-04, 8.5173e-03, 1.2499e-06, 1.8416e-04, 1.1859e-04, 1.6505e-07,\n","         3.7649e-05, 3.2568e-04, 6.7890e-08, 4.7654e-05, 4.1645e-04, 4.3727e-05,\n","         6.3846e-01, 3.7554e-05, 5.4069e-04, 4.6167e-07, 1.4107e-04, 1.0725e-06,\n","         1.0047e-03, 1.2020e-04, 1.5142e-06, 8.2656e-07, 3.5806e-06, 5.0821e-06,\n","         1.6810e-03, 3.6251e-04, 4.8979e-04, 1.3364e-04, 2.5196e-04, 5.3497e-04,\n","         2.9602e-07, 1.2554e-04, 9.8695e-05, 1.1380e-04, 8.1241e-05]],\n","       device='cuda:0')\n","tensor([[1.3574e-04, 2.4571e-04, 1.4362e-05, 1.9698e-05, 6.4896e-07, 4.3812e-08,\n","         1.6165e-06, 1.0416e-04, 3.0976e-04, 4.8237e-05, 3.7370e-05, 2.4307e-04,\n","         1.3019e-04, 2.1381e-03, 1.7567e-08, 6.7743e-04, 3.0614e-05, 2.0716e-06,\n","         5.7431e-05, 4.2520e-06, 7.8653e-07, 3.8053e-04, 2.5886e-05, 8.9036e-05,\n","         6.6842e-05, 9.9031e-01, 1.7033e-04, 3.6988e-04, 4.4896e-06, 4.6373e-05,\n","         1.0205e-07, 6.2022e-06, 5.1047e-06, 4.5072e-07, 1.2470e-04, 7.6900e-06,\n","         1.4892e-05, 2.0565e-05, 6.7589e-07, 9.0028e-04, 8.6758e-04, 1.2720e-04,\n","         5.7147e-06, 1.6508e-04, 3.0840e-04, 1.7723e-03, 1.0975e-05]],\n","       device='cuda:0')\n","tensor([[1.3964e-03, 2.6175e-07, 6.0606e-04, 3.4011e-05, 8.0504e-05, 3.8406e-05,\n","         9.6335e-05, 3.8957e-07, 2.3830e-04, 1.3411e-03, 1.2330e-05, 5.4593e-05,\n","         6.0181e-05, 1.7655e-04, 5.8878e-07, 1.9747e-05, 1.6458e-03, 2.4872e-06,\n","         1.5851e-06, 1.7822e-05, 2.1676e-06, 5.6648e-06, 9.7525e-08, 2.1137e-05,\n","         8.7125e-04, 3.8311e-04, 9.7741e-01, 6.2341e-05, 2.2696e-05, 3.7766e-08,\n","         5.7627e-04, 9.8525e-07, 4.9623e-05, 3.9757e-07, 3.0132e-05, 3.8507e-06,\n","         4.0999e-03, 5.0521e-06, 4.8296e-05, 3.8055e-04, 5.9027e-05, 1.2199e-03,\n","         7.5757e-06, 1.4241e-05, 8.8692e-03, 3.4219e-05, 2.2887e-06]],\n","       device='cuda:0')\n","tensor([[2.2502e-06, 5.2797e-07, 3.8842e-04, 1.8513e-06, 1.3708e-06, 2.3939e-05,\n","         2.4206e-06, 2.5993e-07, 4.4475e-05, 2.5396e-06, 4.9801e-04, 1.6066e-03,\n","         4.1732e-05, 1.5652e-05, 1.3705e-05, 1.9858e-04, 1.6427e-05, 3.2084e-06,\n","         6.0411e-06, 2.4633e-07, 6.9082e-04, 2.7201e-04, 7.1976e-05, 1.3251e-04,\n","         2.1872e-06, 1.0440e-03, 1.1447e-04, 9.8149e-01, 3.3744e-04, 9.3195e-07,\n","         6.6692e-07, 9.8977e-08, 4.8381e-07, 1.3767e-04, 1.6719e-06, 1.3684e-04,\n","         9.5103e-04, 4.7481e-06, 1.1039e-05, 2.3137e-03, 2.6067e-04, 7.8455e-05,\n","         5.3439e-05, 6.1078e-04, 2.2566e-04, 8.1835e-03, 3.5389e-06]],\n","       device='cuda:0')\n","tensor([[3.9427e-05, 3.0676e-06, 3.0399e-05, 3.6087e-04, 4.4024e-06, 3.6430e-02,\n","         1.0244e-04, 8.9192e-09, 4.4716e-04, 1.7942e-04, 1.3468e-06, 4.4121e-05,\n","         2.6514e-04, 1.8771e-04, 1.1731e-05, 3.6545e-05, 2.5450e-04, 2.9969e-06,\n","         1.9174e-05, 1.5797e-03, 1.4640e-06, 2.4515e-05, 8.1057e-09, 6.8086e-05,\n","         7.7889e-05, 4.6542e-08, 9.2661e-06, 4.1477e-06, 9.5775e-01, 4.4408e-07,\n","         3.9808e-06, 1.0831e-06, 3.8313e-07, 6.2761e-06, 3.5816e-06, 1.3718e-05,\n","         3.0636e-05, 1.1832e-04, 4.4125e-04, 4.2211e-04, 5.2814e-05, 7.3371e-04,\n","         6.5020e-06, 2.9920e-05, 9.5461e-05, 9.1247e-05, 1.1987e-05]],\n","       device='cuda:0')\n","tensor([[6.6858e-06, 9.5923e-05, 1.5472e-06, 2.2293e-05, 3.6876e-05, 4.5804e-04,\n","         9.5796e-08, 6.3404e-03, 3.7106e-06, 4.6301e-05, 1.6980e-06, 7.1175e-07,\n","         4.5609e-05, 1.6548e-05, 1.0707e-06, 2.8166e-04, 1.0245e-06, 9.2440e-06,\n","         2.7814e-03, 2.8584e-03, 8.5147e-06, 1.3560e-05, 1.5320e-07, 1.6661e-06,\n","         6.6139e-06, 8.3873e-06, 9.6707e-08, 6.0114e-08, 3.5770e-06, 9.3523e-01,\n","         4.1115e-04, 1.4920e-04, 3.6496e-07, 2.3152e-04, 9.4591e-04, 2.8360e-04,\n","         1.5123e-05, 2.0754e-07, 1.9423e-06, 2.4227e-06, 5.6375e-04, 2.2862e-05,\n","         1.5665e-06, 1.0151e-05, 3.3451e-05, 2.9897e-03, 4.6052e-02]],\n","       device='cuda:0')\n","tensor([[2.5136e-03, 1.5020e-05, 1.0761e-05, 3.0185e-07, 2.1724e-03, 2.3681e-06,\n","         1.1900e-04, 6.8553e-08, 2.1605e-06, 1.4772e-05, 6.3750e-06, 3.9767e-07,\n","         3.4254e-04, 3.7552e-05, 1.8649e-06, 1.4959e-06, 5.3968e-05, 2.1121e-04,\n","         2.0150e-04, 2.1751e-03, 2.3315e-04, 2.6554e-04, 2.9824e-05, 4.1952e-04,\n","         2.1182e-03, 2.9320e-06, 4.2554e-05, 8.4456e-07, 2.7580e-05, 9.1139e-07,\n","         9.7052e-01, 1.3897e-02, 5.0182e-04, 7.0220e-05, 1.5117e-03, 6.2739e-06,\n","         7.0842e-04, 3.9434e-05, 6.9457e-05, 2.3496e-05, 2.2294e-06, 3.6353e-05,\n","         2.6536e-04, 1.2109e-03, 4.6166e-05, 3.9022e-05, 3.0848e-05]],\n","       device='cuda:0')\n","tensor([[1.0163e-04, 1.2959e-04, 3.4795e-06, 1.3456e-07, 8.3579e-05, 2.1831e-05,\n","         6.2247e-05, 2.1489e-06, 1.0498e-05, 1.6692e-07, 4.6010e-08, 8.9954e-07,\n","         2.7851e-05, 3.5039e-05, 1.4829e-07, 3.2499e-06, 5.1662e-06, 9.6497e-06,\n","         2.0418e-04, 1.4827e-03, 3.3902e-05, 2.3291e-04, 9.1337e-06, 2.6372e-04,\n","         3.2202e-05, 8.7655e-06, 6.8291e-07, 1.3774e-07, 6.9043e-06, 1.7223e-05,\n","         3.4207e-02, 9.5087e-01, 1.3064e-04, 1.1735e-04, 7.6744e-03, 2.6553e-06,\n","         2.6639e-06, 3.1333e-05, 1.2918e-05, 1.2118e-05, 5.5398e-06, 1.0555e-06,\n","         1.7487e-06, 1.9039e-04, 8.4135e-07, 3.9152e-03, 3.4424e-05]],\n","       device='cuda:0')\n","tensor([[2.3804e-05, 1.9966e-04, 3.0330e-06, 2.7531e-08, 2.1937e-05, 1.2126e-05,\n","         2.1931e-05, 2.9528e-08, 6.4906e-08, 1.7285e-07, 5.9920e-08, 1.1158e-07,\n","         2.6172e-05, 4.0185e-05, 2.6521e-08, 2.5808e-07, 7.7998e-06, 1.2478e-04,\n","         3.9866e-05, 6.2875e-06, 1.7694e-04, 1.8550e-04, 6.4547e-04, 1.4291e-03,\n","         1.5504e-06, 1.4648e-07, 6.8506e-05, 5.0464e-07, 4.0983e-04, 1.6848e-07,\n","         3.5247e-03, 6.2024e-04, 9.9128e-01, 1.9090e-05, 6.1061e-05, 8.7620e-07,\n","         1.8089e-04, 2.1455e-05, 7.1309e-07, 4.3872e-06, 1.5439e-06, 3.4115e-07,\n","         4.7827e-05, 7.4774e-04, 1.2509e-06, 4.3273e-05, 6.5421e-07]],\n","       device='cuda:0')\n","tensor([[3.6657e-07, 7.2309e-07, 8.1426e-05, 2.9682e-07, 2.5044e-04, 2.1623e-07,\n","         2.7193e-05, 9.7793e-05, 5.3511e-05, 2.8638e-07, 1.7724e-06, 3.1322e-07,\n","         1.2449e-06, 1.1075e-07, 1.0365e-07, 3.3901e-05, 1.3037e-06, 2.3299e-05,\n","         5.0576e-06, 2.5482e-05, 7.4109e-04, 1.0242e-05, 8.8029e-06, 3.0790e-04,\n","         1.4706e-07, 1.3552e-06, 8.3284e-08, 8.2250e-06, 5.1830e-07, 6.5598e-05,\n","         7.5244e-06, 7.8950e-04, 1.4619e-06, 9.9091e-01, 5.3760e-03, 1.6056e-04,\n","         5.3668e-05, 1.3411e-05, 2.6345e-06, 1.5410e-05, 1.4894e-04, 5.8291e-06,\n","         5.6101e-06, 2.4784e-04, 3.3089e-06, 3.7555e-04, 1.3073e-04]],\n","       device='cuda:0')\n","tensor([[9.8274e-07, 8.6703e-05, 8.1112e-06, 7.3341e-06, 1.3893e-02, 1.7275e-05,\n","         4.3083e-06, 9.3714e-04, 2.1340e-05, 1.4008e-04, 3.6725e-07, 7.7083e-07,\n","         7.9451e-07, 1.7149e-05, 1.1103e-07, 2.1855e-05, 3.0843e-06, 2.8177e-05,\n","         5.9134e-05, 3.5926e-04, 1.3400e-04, 9.4753e-05, 6.3215e-06, 5.1676e-05,\n","         4.1967e-07, 5.7878e-05, 3.6406e-06, 5.7737e-08, 1.2659e-05, 3.6309e-04,\n","         1.5671e-03, 6.0570e-03, 1.6801e-05, 1.7281e-03, 9.6931e-01, 4.5185e-05,\n","         2.0953e-05, 1.4440e-05, 7.2436e-06, 5.8973e-06, 4.7796e-05, 5.5771e-04,\n","         4.6134e-05, 2.6034e-05, 1.5164e-04, 2.4600e-03, 1.6092e-03]],\n","       device='cuda:0')\n","tensor([[1.0867e-05, 3.9516e-05, 5.9766e-02, 4.0866e-03, 2.9843e-05, 1.0448e-05,\n","         1.1789e-06, 1.2841e-03, 9.1931e-05, 1.0216e-05, 1.8005e-07, 7.2528e-05,\n","         6.1354e-05, 2.0887e-05, 6.8102e-05, 3.0221e-05, 3.5845e-07, 1.5975e-06,\n","         3.5566e-04, 1.3597e-04, 9.1635e-07, 2.2128e-04, 4.5878e-06, 2.0856e-06,\n","         2.8120e-06, 1.8700e-06, 4.5843e-07, 1.1526e-05, 2.4069e-06, 2.3840e-05,\n","         4.0970e-05, 6.6868e-05, 2.2289e-08, 2.4509e-04, 8.4124e-05, 9.3120e-01,\n","         2.7122e-04, 1.4428e-05, 4.7700e-06, 1.5521e-04, 1.5253e-05, 1.9555e-04,\n","         1.8609e-06, 9.6571e-05, 3.1072e-04, 4.7725e-05, 8.9918e-04]],\n","       device='cuda:0')\n","tensor([[9.5341e-04, 1.1296e-06, 8.9400e-03, 2.4934e-04, 5.8448e-04, 4.2919e-06,\n","         2.3899e-05, 2.7279e-05, 1.9797e-04, 2.3008e-03, 7.3703e-03, 6.2789e-05,\n","         5.1493e-04, 2.9403e-04, 8.3203e-05, 2.2885e-04, 5.8764e-04, 6.6663e-05,\n","         2.8521e-05, 1.2951e-04, 2.0255e-05, 1.4253e-04, 4.6067e-05, 7.3220e-04,\n","         2.5106e-03, 2.9095e-06, 2.0193e-03, 1.0006e-04, 3.0533e-05, 3.6128e-07,\n","         2.1563e-03, 1.4915e-05, 1.5717e-04, 6.7118e-05, 2.8604e-04, 6.7136e-04,\n","         9.4965e-01, 6.5311e-05, 2.7815e-03, 9.0931e-04, 3.2235e-04, 7.9829e-03,\n","         1.5635e-04, 1.1575e-03, 5.2121e-03, 1.1066e-04, 4.8804e-05]],\n","       device='cuda:0')\n","tensor([[6.0480e-04, 1.2024e-04, 7.5939e-05, 2.2994e-04, 9.3669e-06, 2.4834e-04,\n","         1.6959e-02, 1.0334e-07, 1.8322e-04, 2.9793e-06, 2.4855e-06, 6.2872e-03,\n","         1.1436e-05, 6.7730e-04, 2.8954e-05, 1.2240e-05, 1.1851e-03, 3.6785e-06,\n","         7.3936e-04, 2.1171e-04, 3.3376e-04, 7.1756e-04, 3.5083e-07, 4.0670e-04,\n","         2.4071e-04, 2.9828e-05, 1.7121e-05, 7.5197e-06, 3.3536e-04, 2.8350e-07,\n","         4.1568e-05, 2.0697e-05, 3.3577e-05, 1.1405e-04, 2.4749e-05, 5.0004e-05,\n","         1.0333e-03, 9.6571e-01, 6.9909e-05, 1.2669e-04, 3.4049e-04, 3.0845e-05,\n","         2.3558e-03, 4.7232e-05, 6.0879e-05, 2.6689e-05, 2.3550e-04]],\n","       device='cuda:0')\n","tensor([[1.0375e-04, 5.5198e-06, 6.6486e-04, 2.8581e-06, 4.3354e-05, 2.7993e-06,\n","         2.6570e-04, 9.4949e-06, 9.0094e-05, 7.3099e-07, 1.2304e-05, 7.5781e-06,\n","         2.8271e-05, 2.2587e-03, 1.0095e-06, 3.4836e-06, 3.4848e-05, 5.7166e-05,\n","         6.4584e-05, 3.0997e-03, 1.5179e-06, 3.1696e-04, 5.3664e-06, 4.8739e-04,\n","         2.4250e-04, 2.8999e-05, 8.7622e-05, 9.5452e-06, 6.5269e-05, 4.1744e-06,\n","         1.6775e-04, 2.6770e-04, 6.5190e-05, 2.9691e-05, 1.2890e-04, 1.0128e-05,\n","         1.8256e-03, 1.9932e-04, 9.8795e-01, 7.5505e-05, 2.3157e-05, 8.3630e-05,\n","         7.1253e-05, 1.9099e-04, 1.6802e-05, 8.2466e-04, 6.4908e-05]],\n","       device='cuda:0')\n","tensor([[7.7599e-05, 9.2742e-07, 1.3512e-04, 2.3425e-06, 6.4785e-06, 2.3208e-06,\n","         1.4177e-05, 3.1505e-07, 1.2096e-04, 3.3790e-05, 8.9327e-07, 7.2103e-05,\n","         8.8673e-03, 1.4309e-06, 1.6464e-03, 7.4700e-05, 2.4174e-05, 1.3489e-05,\n","         6.0386e-05, 8.9769e-07, 2.6649e-05, 4.7026e-04, 3.7560e-07, 6.1852e-06,\n","         3.0482e-04, 2.6246e-04, 2.0889e-05, 3.7906e-04, 2.3128e-05, 8.7863e-06,\n","         4.4307e-04, 1.1318e-06, 4.5817e-07, 2.1028e-05, 3.9882e-06, 5.1233e-05,\n","         4.5097e-04, 5.7245e-06, 1.4116e-05, 9.8390e-01, 4.9759e-05, 2.0660e-04,\n","         3.7110e-04, 9.9570e-06, 6.0372e-05, 8.1365e-04, 9.3860e-04]],\n","       device='cuda:0')\n","tensor([[1.2928e-06, 7.7983e-05, 4.5904e-05, 3.1192e-05, 1.0423e-04, 3.8335e-04,\n","         2.1946e-04, 5.9823e-05, 4.3092e-04, 6.5240e-05, 5.3914e-05, 6.8176e-05,\n","         2.7811e-04, 3.4747e-06, 2.2600e-04, 1.9860e-01, 1.8172e-04, 8.4700e-05,\n","         8.7210e-04, 1.3564e-03, 5.7454e-04, 7.0571e-04, 4.6953e-06, 6.8653e-06,\n","         4.5740e-06, 1.6812e-03, 3.0076e-06, 5.4107e-05, 6.3163e-04, 3.5045e-04,\n","         1.7134e-06, 6.1821e-06, 9.6015e-06, 2.3063e-04, 7.4450e-04, 1.5186e-04,\n","         6.7923e-05, 2.7861e-04, 8.8584e-05, 2.0772e-04, 7.7550e-01, 3.0406e-04,\n","         6.8995e-04, 9.9607e-06, 4.3028e-04, 1.0381e-03, 1.3086e-02]],\n","       device='cuda:0')\n","tensor([[4.6068e-04, 9.8552e-05, 6.7628e-04, 6.3065e-04, 6.5910e-04, 8.9933e-04,\n","         9.0348e-04, 3.4787e-04, 1.1912e-02, 9.5301e-02, 4.9726e-05, 2.6069e-04,\n","         2.8683e-04, 6.7066e-05, 2.8471e-04, 3.3963e-04, 1.3261e-02, 1.3953e-05,\n","         8.5402e-05, 1.0874e-03, 1.2491e-04, 5.6922e-05, 3.2423e-06, 6.9597e-05,\n","         8.0096e-04, 8.8711e-05, 1.2979e-03, 8.3473e-06, 5.2216e-03, 4.4166e-05,\n","         2.1086e-04, 1.2892e-05, 3.0127e-06, 1.3428e-04, 2.1487e-03, 4.7441e-04,\n","         8.9733e-03, 2.0131e-04, 4.7937e-04, 5.8097e-04, 7.6209e-04, 7.4808e-01,\n","         8.1193e-05, 4.6798e-04, 1.0152e-01, 4.7347e-04, 5.1650e-05]],\n","       device='cuda:0')\n","tensor([[1.5849e-07, 5.2932e-05, 3.1772e-04, 2.1128e-07, 2.4557e-04, 3.7326e-06,\n","         4.0136e-04, 3.2741e-07, 9.7403e-07, 1.8866e-06, 1.0257e-04, 5.8826e-07,\n","         1.6033e-07, 1.7896e-06, 1.7886e-07, 1.6202e-05, 4.9422e-05, 3.8262e-03,\n","         3.9505e-05, 2.1739e-05, 1.8401e-03, 3.3862e-03, 2.0829e-04, 4.3668e-05,\n","         1.8079e-07, 1.7271e-07, 3.0656e-06, 6.6435e-05, 4.1860e-04, 8.4193e-06,\n","         3.3218e-04, 3.8933e-05, 8.4737e-05, 9.9159e-05, 4.9166e-05, 1.0355e-05,\n","         7.0173e-05, 1.6593e-03, 2.4610e-05, 2.4133e-06, 3.8726e-04, 6.9960e-06,\n","         9.7175e-01, 1.2714e-02, 5.9643e-05, 5.7106e-04, 1.0777e-03]],\n","       device='cuda:0')\n","tensor([[3.9093e-04, 1.5355e-04, 4.8743e-05, 1.0987e-05, 5.7821e-05, 2.0598e-06,\n","         5.6909e-07, 9.0838e-04, 3.7181e-07, 6.0035e-05, 3.3620e-04, 1.1966e-05,\n","         1.4779e-04, 4.1672e-04, 8.7433e-08, 3.7266e-06, 5.0449e-07, 1.4403e-04,\n","         1.4204e-04, 6.5965e-05, 1.3599e-04, 2.2845e-04, 2.8578e-03, 1.8949e-02,\n","         5.4719e-04, 8.2436e-05, 7.4810e-06, 1.9016e-04, 8.1933e-07, 5.9050e-05,\n","         2.6251e-04, 6.2260e-04, 4.5124e-04, 4.9168e-04, 3.3442e-04, 2.7538e-05,\n","         9.6426e-04, 4.6484e-05, 1.3388e-04, 4.2864e-04, 2.0023e-05, 1.1340e-04,\n","         3.5974e-03, 9.6160e-01, 4.3410e-05, 4.8190e-03, 8.0966e-05]],\n","       device='cuda:0')\n","tensor([[7.4050e-04, 1.1077e-04, 4.1063e-04, 3.2030e-04, 1.9783e-03, 1.8167e-04,\n","         2.8287e-05, 5.6858e-04, 8.2387e-03, 1.7157e-01, 3.8732e-05, 1.7374e-04,\n","         2.9826e-05, 1.8133e-04, 1.9334e-04, 3.9852e-04, 3.6488e-04, 3.2311e-06,\n","         3.1697e-04, 1.8393e-04, 1.9540e-06, 1.2672e-04, 3.6866e-05, 3.0536e-05,\n","         7.6652e-04, 8.3708e-04, 1.8174e-02, 5.2883e-05, 2.9982e-04, 6.1242e-05,\n","         1.1723e-04, 2.4026e-04, 9.4156e-06, 9.7008e-05, 8.5251e-04, 3.7270e-04,\n","         1.1544e-02, 1.1443e-04, 1.2121e-05, 8.3319e-04, 9.1765e-04, 7.1517e-02,\n","         1.5735e-04, 5.9044e-04, 7.0591e-01, 1.1770e-04, 1.8139e-04]],\n","       device='cuda:0')\n","tensor([[1.7543e-04, 4.3308e-04, 1.1317e-04, 4.7568e-07, 3.4459e-05, 1.7649e-04,\n","         3.8737e-05, 1.6485e-05, 8.7395e-05, 8.8734e-06, 1.4890e-05, 1.6183e-05,\n","         1.8852e-03, 1.5880e-04, 9.9053e-07, 4.6930e-04, 3.1580e-06, 2.1762e-05,\n","         1.0085e-03, 3.1504e-05, 1.4848e-04, 5.3322e-04, 1.2276e-05, 3.4186e-04,\n","         1.9986e-04, 1.4304e-04, 1.3618e-06, 2.8885e-03, 1.1249e-04, 1.7965e-03,\n","         4.7759e-06, 5.8441e-03, 3.1472e-05, 4.1722e-04, 2.3800e-03, 1.3047e-05,\n","         1.1176e-04, 1.9645e-05, 4.7563e-05, 7.2250e-04, 6.7722e-04, 5.2035e-05,\n","         7.2910e-05, 2.3388e-03, 1.6330e-05, 9.7518e-01, 1.1965e-03]],\n","       device='cuda:0')\n","tensor([[1.3034e-04, 3.9599e-04, 3.2477e-05, 6.5153e-07, 6.0161e-04, 3.3841e-05,\n","         6.4693e-06, 3.1073e-04, 1.5007e-05, 1.3977e-05, 2.3634e-05, 1.9007e-06,\n","         6.0179e-04, 1.6752e-06, 1.5446e-04, 1.0672e-03, 2.1949e-06, 2.0504e-04,\n","         8.0850e-04, 8.4099e-04, 6.5218e-05, 4.3410e-04, 2.6353e-07, 7.2879e-06,\n","         3.0739e-04, 2.7956e-05, 1.9048e-06, 1.4348e-05, 2.8062e-06, 2.8568e-02,\n","         4.8664e-06, 4.7783e-05, 1.2678e-06, 9.6610e-04, 4.9082e-03, 1.6160e-04,\n","         1.8710e-05, 7.5874e-05, 5.9572e-05, 8.8428e-04, 1.2152e-02, 1.6875e-04,\n","         4.7867e-04, 6.7575e-06, 5.9591e-05, 1.3399e-03, 9.4399e-01]],\n","       device='cuda:0')\n"]}]},{"cell_type":"code","source":["!python emnist_0.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FqbP22m7LhBh","executionInfo":{"status":"ok","timestamp":1673450453404,"user_tz":-480,"elapsed":9036,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"d67108a1-0f23-4750-a7a3-c75c2e0e8828"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"emnist_0.py\", line 434, in <module>\n","    main()\n","  File \"emnist_0.py\", line 416, in main\n","    model = model_root().to(torch.device(\"cuda\"))\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 987, in to\n","    return self._apply(convert)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 639, in _apply\n","    module._apply(fn)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 639, in _apply\n","    module._apply(fn)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 662, in _apply\n","    param_applied = fn(param)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 985, in convert\n","    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\", line 229, in _lazy_init\n","    torch._C._cuda_init()\n","RuntimeError: No CUDA GPUs are available\n"]}]},{"cell_type":"code","source":["!python emnist_root.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gaiDoC9Er2x","executionInfo":{"status":"ok","timestamp":1672036999765,"user_tz":-480,"elapsed":4821,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"5706667b-143d-42bb-e4ea-3f94186e67e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"emnist_root.py\", line 269, in <module>\n","    main()\n","  File \"emnist_root.py\", line 255, in main\n","    model.load_state_dict(torch.load('/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Train/emnist_root.py'))\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 795, in load\n","    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 1002, in _legacy_load\n","    magic_number = pickle_module.load(f, **pickle_load_args)\n","_pickle.UnpicklingError: could not find MARK\n"]}]},{"cell_type":"code","source":["!python emnist_root.py"],"metadata":{"id":"I-x6yr3UKpSK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##2.2 Run the Inference"],"metadata":{"id":"o_bUQs2KEdcq"}},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Test/\n","!python test_emnist.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jr_f2bFulc4s","executionInfo":{"status":"ok","timestamp":1672035135609,"user_tz":-480,"elapsed":36940,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"63306ae0-973d-48b2-e356-8fb9b118c35b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Test\n","Val:  0.913386524822695\n"]}]},{"cell_type":"markdown","source":["##2.3 Demo"],"metadata":{"id":"2nTNvpvOIrSC"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torchvision import models\n","from torch.autograd import Variable\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import time\n","import numpy as np\n","import shutil\n","import os\n","import argparse\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import glob\n","import os, random"],"metadata":{"id":"cIUOUvxKIuqB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m = nn.Softmax()\n","\n","cfg = {\n","    '0_0': [32, 48, 'M'],\n","    '0_3': [32, 32, 'M'],\n","    '0_5': [32, 32, 'M'],\n","    '0': [16, 32, 'M', 'D'],\n","    '1': [16, 32, 'M', 'D'],\n","    '1_0': [32, 64, 'M'],\n","    'root': [16, 'M', 16, 'D'],\n","    '3': [16, 32, 'M'],\n","    '3_0': [32, 32, 'M'],\n","    '4': [16, 32,'M'],\n","    '6': [16, 32, 'M'],\n","    '8': [16, 32, 'M'],\n","    '8_0': [32, 32, 'M'],\n","    '9': [16, 32, 'M'],\n","    '10': [16, 32, 'M'],\n","}"],"metadata":{"id":"QP45vDYPIx2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class model(nn.Module):\n","    def __init__(self, size):\n","        super(model, self).__init__()\n","        self.features = self._make_layers(cfg[size])\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(16*14*14, 14),\n","                )\n","\n","    def forward(self, x):\n","        y = self.features(x)\n","        x = y.view(y.size(0), -1)\n","        out = self.classifier(x)\n","        return y,out\n","\n","    def _make_layers(self, cfg):\n","        layers = []\n","        in_channels = 1\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_root():\n","    return model('root')\n","\n","class mod_zero(nn.Module):\n","    def __init__(self, size):\n","        super(mod_zero, self).__init__()\n","        self.features = self._make_layers(cfg[size], 16)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*7*7, 8),\n","                )\n","\n","    def forward(self, x):\n","        y = self.features(x)\n","        x = y.view(y.size(0), -1)\n","        out = self.classifier(x)\n","        return y,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_0():\n","    return mod_zero('0')\n","\n","\n","class mod_zero_three(nn.Module):\n","    def __init__(self, size):\n","        super(mod_zero_three, self).__init__()\n","        self.features = self._make_layers(cfg[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*3*3, 2),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_0_3():\n","    return mod_zero_three('0_3')\n","\n","class mod_zero_zero(nn.Module):\n","    def __init__(self, size):\n","        super(mod_zero_zero, self).__init__()\n","        self.features = self._make_layers(cfg[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(48*3*3, 2),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_0_0():\n","    return mod_zero_zero('0_0')\n","\n","class mod_zero_five(nn.Module):\n","    def __init__(self, size):\n","        super(mod_zero_five, self).__init__()\n","        self.features = self._make_layers(cfg[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*3*3, 3),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_0_5():\n","    return mod_zero_five('0_5')\n","\n","class mod_one(nn.Module):\n","    def __init__(self, size):\n","        super(mod_one, self).__init__()\n","        self.features = self._make_layers(cfg[size], 16)\n","        #self.features_down = self._make_layers(cfg_down[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*7*7, 2),\n","                )\n","\n","    def forward(self, x):\n","        y = self.features(x)\n","        #x  = self.features_down(y)\n","        x = y.view(y.size(0), -1)\n","        out = self.classifier(x)\n","        return y,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_1():\n","    return mod_one('1')\n","\n","\n","class mod_one_zero(nn.Module):\n","    def __init__(self, size):\n","        super(mod_one_zero, self).__init__()\n","        self.features = self._make_layers(cfg[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(64*3*3, 3),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_1_0():\n","    return mod_one_zero('1_0')\n","\n","class mod_three(nn.Module):\n","    def __init__(self, size):\n","        super(mod_three, self).__init__()\n","        self.features = self._make_layers(cfg[size], 16)\n","        #self.features_down = self._make_layers(cfg_down[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*7*7, 13),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        #x  = self.features_down(y)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_3():\n","    return mod_three('3')\n","\n","\n","class mod_three_zero(nn.Module):\n","    def __init__(self, size):\n","        super(mod_three_zero, self).__init__()\n","        self.features = self._make_layers(cfg[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*3*3, 2),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_3_0():\n","    return mod_three_zero('3_0')\n","\n","class mod_four(nn.Module):\n","    def __init__(self, size):\n","        super(mod_four, self).__init__()\n","        self.features = self._make_layers(cfg[size], 16)\n","        #self.features_down = self._make_layers(cfg_down[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*7*7, 2),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        #x  = self.features_down(y)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_4():\n","    return mod_four('4')\n","\n","class mod_six(nn.Module):\n","    def __init__(self, size):\n","        super(mod_six, self).__init__()\n","        self.features = self._make_layers(cfg[size], 16)\n","        #self.features_down = self._make_layers(cfg_down[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*7*7, 2),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        #x  = self.features_down(y)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_6():\n","    return mod_six('6')\n","\n","class mod_eight(nn.Module):\n","    def __init__(self, size):\n","        super(mod_eight, self).__init__()\n","        self.features = self._make_layers(cfg[size], 16)\n","        #self.features_down = self._make_layers(cfg_down[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*7*7, 3),\n","                )\n","\n","    def forward(self, x):\n","        y = self.features(x)\n","        #x  = self.features_down(y)\n","        x = y.view(y.size(0), -1)\n","        out = self.classifier(x)\n","        return y,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_8():\n","    return mod_eight('8')\n","\n","class mod_eight_zero(nn.Module):\n","    def __init__(self, size):\n","        super(mod_eight_zero, self).__init__()\n","        self.features = self._make_layers(cfg[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*3*3, 2),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_8_0():\n","    return mod_eight_zero('8_0')\n","\n","class mod_nine(nn.Module):\n","    def __init__(self, size):\n","        super(mod_nine, self).__init__()\n","        self.features = self._make_layers(cfg[size], 16)\n","        #self.features_down = self._make_layers(cfg_down[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*7*7, 2),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        #x  = self.features_down(y)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_9():\n","    return mod_nine('9')\n","\n","class mod_ten(nn.Module):\n","    def __init__(self, size):\n","        super(mod_ten, self).__init__()\n","        self.features = self._make_layers(cfg[size], 16)\n","        #self.features_down = self._make_layers(cfg_down[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*7*7, 2),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        #x  = self.features_down(y)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_10():\n","    return mod_ten('10')"],"metadata":{"id":"K2KfVOvSI0R6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["leaves = {\"root\": {2:3, 5:7, 7:14, 11:25, 12:38, 13:45}, \"root_0\":{1:13,2:36,4:26,6:8,7:11}, \"root_0_0\": {0:0,1:24},\n","          \"root_0_3\": {0:2,1:35}, \"root_0_5\": {0:44,1:41,2:9}, \"root_1\": {1:19}, \"root_1_0\": {0:1,1:18,2:21},\n","         \"root_8\": {1:46,2:29}, \"root_8_0\": {0:15,1:40}, \"root_10\": {0:23, 1:32},\n","          \"root_3\":dict(zip(range(1,12), [17, 30, 10, 31, 33, 20, 27, 42, 37, 6, 16])),\n","         \"root_3_0\":{0:4,1:34}, \"root_4\":{0:5, 1:28}, \"root_6\":{0:12, 1:39}, \"root_9\": {0:22, 1:43}, }\n","\n","def is_leaf(path, output):\n","    if output in leaves[path].keys():\n","        return True\n","    return False\n","\n","def find_category(path, leaf):\n","    return leaves[path][leaf]"],"metadata":{"id":"vL-9jwoRI3YT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = {\"root_0\": model_0(), \"root_0_0\": model_0_0(),\n","            \"root_0_3\": model_0_3(), \"root_0_5\": model_0_5(), \"root_1\": model_1(), \"root_1_0\": model_1_0(), \n","            \"root_8\": model_8(), \"root_8_0\": model_8_0(), \"root_10\": model_10(),\n","            \"root_3\": model_3(), \"root_3_0\": model_3_0(), \"root_4\":model_4(), \"root_6\": model_6(),\n","            \"root_9\": model_9()}\n","\n","model_name = {\"root_0\":\"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_0.pth\", \"root_0_0\": \"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_0_0.pth\",\n","            \"root_0_3\": \"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_0_3.pth\", \"root_0_5\": \"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_0_5.pth\", \"root_1\": \"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_1.pth\", \"root_1_0\": \"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_1_0.pth\", \n","            \"root_8\": \"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_8.pth\", \"root_8_0\": \"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_8_0.pth\", \"root_10\": \"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_10.pth\",\n","            \"root_3\":\"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_3.pth\", \"root_3_0\":\"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_3_0.pth\", \"root_4\":\"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_4.pth\", \"root_6\":\"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_6.pth\",\n","            \"root_9\": \"/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_9.pth\" }\n","\n","def fetch_model(path):\n","    model = models[path].cuda().eval()\n","    model.load_state_dict(torch.load(model_name[path]))\n","    return model\n","    "],"metadata":{"id":"ttZOWlfvI6Dy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mapper = dict(zip(list(range(0, 47)), [0,1,2,3,4,5,6,7,8,9,'A','B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'd', 'e', 'f', 'g', 'h', 'i', 'n', 'q', 'r', 't',]))\n","def demo(model, img, device):\n","    parent = \"root\"\n","    path = \"root\"\n","    next_data, root_out = model(img.view(1,img.shape[0], img.shape[1], img.shape[2]))\n","    root_out = root_out.max(1, keepdim=True)[1]\n","    if is_leaf(path, root_out.item()):\n","            return mapper[find_category(path, root_out.item())]\n","    parent = str(root_out.item())\n","    \n","    while(1): \n","        path = path + \"_\" + parent\n","        model = fetch_model(path)\n","        model = model.eval()\n","        next_data, model_out = model(next_data)\n","        model_out = model_out.max(1, keepdim=True)[1]\n","        if is_leaf(path, model_out.item()):\n","            return mapper[find_category(path, model_out.item())]\n","        parent = str(model_out.item())"],"metadata":{"id":"DkAjET-fI7-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    img_path = glob.glob('/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Demo/img/emnist/*.jpg')[random.randint(1,len(glob.glob('/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Demo/img/emnist/*.jpg'))-1)]\n","    img = Image.open(img_path)\n","    plt.imshow(np.asarray(img.rotate(-90).transpose(Image.FLIP_LEFT_RIGHT)))\n","    plt.axis('off')\n","    plt.title(\"Input Image\", fontsize = 20)\n","    plt.show()\n","    img = transforms.ToTensor()(img).cuda()\n","    model = model_root().cuda().eval()\n","    model.load_state_dict(torch.load('/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/EMNIST/Models/emnist_root.pth'))\n","    print(\"Prediction: \", demo(model, img, torch.device(\"cuda\")))\n","\n","if __name__== \"__main__\":\n","        main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"id":"QNptgGRzI9sJ","executionInfo":{"status":"ok","timestamp":1672036708740,"user_tz":-480,"elapsed":6382,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"c6aed82f-d5be-44a3-8a5f-6e17b69bd7c2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Prediction:  e\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rgauJVfUJAqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xSysI9qaJB-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3mm9qHupJCDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pdqElWdLI94A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#3. SVHN Dataset"],"metadata":{"id":"FzlyTBMBPLxw"}},{"cell_type":"markdown","source":["## 3.0 Grouper"],"metadata":{"id":"3M0hhRftKR8L"}},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/SVHN/SoftmaxOutput/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4OuFPyQOKUz0","executionInfo":{"status":"ok","timestamp":1684384882581,"user_tz":-480,"elapsed":248,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"6e653549-7d0c-4ccc-cc95-32d5390bab7d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/SVHN/SoftmaxOutput\n"]}]},{"cell_type":"code","source":["!python grouper.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kQDRI8mwKaTL","executionInfo":{"status":"ok","timestamp":1684384895630,"user_tz":-480,"elapsed":4117,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"0fa4b6e1-761e-46f2-f478-47efed0db1a7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["root: \n","[0]\n","[1, 7]\n","[2]\n","[3, 5, 8, 6]\n","[4]\n","[9]\n"]}]},{"cell_type":"markdown","source":["##3.1 Run the Training"],"metadata":{"id":"kD0uy9aaPPXR"}},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/SVHN/Train/"],"metadata":{"id":"_RcxK9UuPOcA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673949517021,"user_tz":-480,"elapsed":1661,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"3a37aae3-cf17-4457-b031-4f6123f2159f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/SVHN/Train\n"]}]},{"cell_type":"code","source":["!python svhn_root.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbNo5xplaBe7","executionInfo":{"status":"ok","timestamp":1673465904556,"user_tz":-480,"elapsed":14754747,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"3f301dfa-c41f-435a-d3c8-1be1a9660fb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using downloaded and verified file: ../dataSVHN/extra_32x32.mat\n","tcmalloc: large alloc 1631641600 bytes == 0x6420000 @  0x7fe641b5f1e7 0x4d30a0 0x5dede2 0x7fe5b9d2b19a 0x7fe5b9d4abe6 0x7fe5b9d56159 0x7fe5b9d4c5af 0x7fe5b9d549fb 0x7fe5b9d4a6d1 0x4f9336 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997c7 0x55d078 0x5d8941 0x5da107 0x586de6 0x5d8cdf 0x55ea20 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630\n","Using downloaded and verified file: ../dataSVHN/train_32x32.mat\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 32, 32]             448\n","       BatchNorm2d-2           [-1, 16, 32, 32]              32\n","              ReLU-3           [-1, 16, 32, 32]               0\n","            Conv2d-4           [-1, 32, 32, 32]           4,640\n","       BatchNorm2d-5           [-1, 32, 32, 32]              64\n","              ReLU-6           [-1, 32, 32, 32]               0\n","            Conv2d-7           [-1, 32, 32, 32]           9,248\n","       BatchNorm2d-8           [-1, 32, 32, 32]              64\n","              ReLU-9           [-1, 32, 32, 32]               0\n","        MaxPool2d-10           [-1, 32, 16, 16]               0\n","        AvgPool2d-11           [-1, 32, 16, 16]               0\n","           Linear-12                    [-1, 6]          49,158\n","================================================================\n","Total params: 63,654\n","Trainable params: 63,654\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 2.00\n","Params size (MB): 0.24\n","Estimated Total Size (MB): 2.25\n","----------------------------------------------------------------\n","None\n"," 1088/1088 [================================================================================>]  Step: 1s133ms | Tot: 1m52s | Loss: 0.0000 | Acc: 97.545% (530596/543950)\n","Epoch:  1 \n","Train:  0.9754499494438827\n","Val:  0.9704656011118833\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 984ms | Tot: 1m57s | Loss: 0.0001 | Acc: 97.655% (531197/543950)\n","Epoch:  2 \n","Train:  0.9765548304072066\n","Val:  0.9688441047023396\n"," 1088/1088 [================================================================================>]  Step: 760ms | Tot: 1m54s | Loss: 0.0001 | Acc: 97.630% (531056/543950)\n","Epoch:  3 \n","Train:  0.9762956154058278\n","Val:  0.9687448294119594\n"," 1088/1088 [================================================================================>]  Step: 636ms | Tot: 1m55s | Loss: 0.0001 | Acc: 97.698% (531426/543950)\n","Epoch:  4 \n","Train:  0.976975824983914\n","Val:  0.9715907210695258\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 532ms | Tot: 1m53s | Loss: 0.0001 | Acc: 97.670% (531278/543950)\n","Epoch:  5 \n","Train:  0.9767037411526794\n","Val:  0.9714583540156855\n"," 1088/1088 [================================================================================>]  Step: 505ms | Tot: 1m53s | Loss: 0.0001 | Acc: 97.663% (531236/543950)\n","Epoch:  6 \n","Train:  0.9766265281735453\n","Val:  0.9716072669512559\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 413ms | Tot: 1m53s | Loss: 0.0001 | Acc: 97.677% (531315/543950)\n","Epoch:  7 \n","Train:  0.9767717621104881\n","Val:  0.969886495251332\n"," 1088/1088 [================================================================================>]  Step: 391ms | Tot: 1m55s | Loss: 0.0001 | Acc: 97.661% (531229/543950)\n","Epoch:  8 \n","Train:  0.9766136593436897\n","Val:  0.9700684999503624\n"," 1088/1088 [================================================================================>]  Step: 359ms | Tot: 1m52s | Loss: 0.0001 | Acc: 97.705% (531464/543950)\n","Epoch:  9 \n","Train:  0.9770456843459877\n","Val:  0.9701346834772825\n"," 1088/1088 [================================================================================>]  Step: 360ms | Tot: 1m52s | Loss: 0.0001 | Acc: 97.679% (531326/543950)\n","Epoch:  10 \n","Train:  0.9767919845574041\n","Val:  0.972120189284887\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 341ms | Tot: 1m52s | Loss: 0.0001 | Acc: 97.681% (531336/543950)\n","Epoch:  11 \n","Train:  0.9768103686000551\n","Val:  0.9697210364340316\n"," 1088/1088 [================================================================================>]  Step: 326ms | Tot: 1m54s | Loss: 0.0001 | Acc: 97.659% (531215/543950)\n","Epoch:  12 \n","Train:  0.9765879216839783\n","Val:  0.9707799728647539\n"," 1088/1088 [================================================================================>]  Step: 321ms | Tot: 1m52s | Loss: 0.0001 | Acc: 97.679% (531325/543950)\n","Epoch:  13 \n","Train:  0.9767901461531391\n","Val:  0.9700684999503624\n"," 1088/1088 [================================================================================>]  Step: 322ms | Tot: 1m52s | Loss: 0.0001 | Acc: 97.691% (531389/543950)\n","Epoch:  14 \n","Train:  0.9769078040261053\n","Val:  0.9678678976802674\n"," 1088/1088 [================================================================================>]  Step: 335ms | Tot: 1m52s | Loss: 0.0001 | Acc: 97.671% (531282/543950)\n","Epoch:  15 \n","Train:  0.9767110947697398\n","Val:  0.9702339587676627\n"," 1088/1088 [================================================================================>]  Step: 334ms | Tot: 1m54s | Loss: 0.0001 | Acc: 97.699% (531434/543950)\n","Epoch:  16 \n","Train:  0.9769905322180348\n","Val:  0.9710116152089745\n"," 1088/1088 [================================================================================>]  Step: 323ms | Tot: 1m52s | Loss: 0.0001 | Acc: 97.703% (531456/543950)\n","Epoch:  17 \n","Train:  0.9770309771118669\n","Val:  0.9721863728118071\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 326ms | Tot: 1m51s | Loss: 0.0001 | Acc: 97.669% (531269/543950)\n","Epoch:  18 \n","Train:  0.9766871955142936\n","Val:  0.9704656011118833\n"," 1088/1088 [================================================================================>]  Step: 524ms | Tot: 1m54s | Loss: 0.0001 | Acc: 97.685% (531357/543950)\n","Epoch:  19 \n","Train:  0.9768489750896222\n","Val:  0.9681988153148682\n"," 1088/1088 [================================================================================>]  Step: 309ms | Tot: 1m53s | Loss: 0.0001 | Acc: 97.706% (531470/543950)\n","Epoch:  20 \n","Train:  0.9770567147715783\n","Val:  0.9666931400774347\n"," 1088/1088 [================================================================================>]  Step: 317ms | Tot: 1m52s | Loss: 0.0001 | Acc: 97.728% (531594/543950)\n","Epoch:  21 \n","Train:  0.9772846769004504\n","Val:  0.973046758661769\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 309ms | Tot: 1m52s | Loss: 0.0001 | Acc: 97.709% (531489/543950)\n","Epoch:  22 \n","Train:  0.9770916444526151\n","Val:  0.9726827492637082\n"," 1088/1088 [================================================================================>]  Step: 324ms | Tot: 1m54s | Loss: 0.0001 | Acc: 97.698% (531430/543950)\n","Epoch:  23 \n","Train:  0.9769831786009744\n","Val:  0.9688937423475297\n"," 1088/1088 [================================================================================>]  Step: 318ms | Tot: 1m52s | Loss: 0.0001 | Acc: 97.714% (531516/543950)\n","Epoch:  24 \n","Train:  0.9771412813677728\n","Val:  0.9721532810483471\n"," 1088/1088 [================================================================================>]  Step: 314ms | Tot: 1m51s | Loss: 0.0001 | Acc: 97.685% (531357/543950)\n","Epoch:  25 \n","Train:  0.9768489750896222\n","Val:  0.9719381845858566\n"," 1088/1088 [================================================================================>]  Step: 310ms | Tot: 1m51s | Loss: 0.0001 | Acc: 97.703% (531453/543950)\n","Epoch:  26 \n","Train:  0.9770254618990716\n","Val:  0.9695886693801913\n"," 1088/1088 [================================================================================>]  Step: 313ms | Tot: 1m54s | Loss: 0.0000 | Acc: 97.726% (531579/543950)\n","Epoch:  27 \n","Train:  0.977257100836474\n","Val:  0.9696713987888415\n"," 1088/1088 [================================================================================>]  Step: 325ms | Tot: 1m51s | Loss: 0.0001 | Acc: 97.702% (531448/543950)\n","Epoch:  28 \n","Train:  0.9770162698777461\n","Val:  0.9656011118832523\n"," 1088/1088 [================================================================================>]  Step: 317ms | Tot: 1m51s | Loss: 0.0001 | Acc: 97.736% (531633/543950)\n","Epoch:  29 \n","Train:  0.9773563746667893\n","Val:  0.9712928951983851\n"," 1088/1088 [================================================================================>]  Step: 306ms | Tot: 1m53s | Loss: 0.0000 | Acc: 97.719% (531545/543950)\n","Epoch:  30 \n","Train:  0.9771945950914606\n","Val:  0.9678017141533473\n"," 1088/1088 [================================================================================>]  Step: 316ms | Tot: 1m52s | Loss: 0.0001 | Acc: 98.650% (536609/543950)\n","Epoch:  1 \n","Train:  0.9865042742899164\n","Val:  0.9751315397597538\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 311ms | Tot: 1m51s | Loss: 0.0001 | Acc: 98.661% (536665/543950)\n","Epoch:  2 \n","Train:  0.9866072249287619\n","Val:  0.974767530361693\n"," 1088/1088 [================================================================================>]  Step: 307ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.669% (536708/543950)\n","Epoch:  3 \n","Train:  0.986686276312161\n","Val:  0.9747178927165029\n"," 1088/1088 [================================================================================>]  Step: 308ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.665% (536687/543950)\n","Epoch:  4 \n","Train:  0.986647669822594\n","Val:  0.9744862503722823\n"," 1088/1088 [================================================================================>]  Step: 316ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.678% (536761/543950)\n","Epoch:  5 \n","Train:  0.9867837117382112\n","Val:  0.9750488103511036\n"," 1088/1088 [================================================================================>]  Step: 303ms | Tot: 1m53s | Loss: 0.0001 | Acc: 98.683% (536787/543950)\n","Epoch:  6 \n","Train:  0.9868315102491038\n","Val:  0.9751480856414838\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 302ms | Tot: 1m51s | Loss: 0.0001 | Acc: 98.691% (536828/543950)\n","Epoch:  7 \n","Train:  0.9869068848239728\n","Val:  0.973923690393461\n"," 1088/1088 [================================================================================>]  Step: 311ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.673% (536730/543950)\n","Epoch:  8 \n","Train:  0.9867267212059933\n","Val:  0.9751149938780238\n"," 1088/1088 [================================================================================>]  Step: 311ms | Tot: 1m53s | Loss: 0.0001 | Acc: 98.692% (536834/543950)\n","Epoch:  9 \n","Train:  0.9869179152495634\n","Val:  0.9747509844799629\n"," 1088/1088 [================================================================================>]  Step: 311ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.695% (536849/543950)\n","Epoch:  10 \n","Train:  0.9869454913135398\n","Val:  0.9750653562328336\n"," 1088/1088 [================================================================================>]  Step: 313ms | Tot: 1m51s | Loss: 0.0001 | Acc: 98.706% (536911/543950)\n","Epoch:  11 \n","Train:  0.9870594723779759\n","Val:  0.9734769515867501\n"," 1088/1088 [================================================================================>]  Step: 318ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.690% (536824/543950)\n","Epoch:  12 \n","Train:  0.9868995312069124\n","Val:  0.9748833515338032\n"," 1088/1088 [================================================================================>]  Step: 318ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.718% (536976/543950)\n","Epoch:  13 \n","Train:  0.9871789686552073\n","Val:  0.9744531586088223\n"," 1088/1088 [================================================================================>]  Step: 306ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.722% (536996/543950)\n","Epoch:  14 \n","Train:  0.9872157367405092\n","Val:  0.9751480856414838\n"," 1088/1088 [================================================================================>]  Step: 304ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.729% (537039/543950)\n","Epoch:  15 \n","Train:  0.9872947881239085\n","Val:  0.9741056950924915\n"," 1088/1088 [================================================================================>]  Step: 318ms | Tot: 1m51s | Loss: 0.0001 | Acc: 98.726% (537021/543950)\n","Epoch:  16 \n","Train:  0.9872616968471367\n","Val:  0.9747344385982329\n"," 1088/1088 [================================================================================>]  Step: 302ms | Tot: 1m53s | Loss: 0.0001 | Acc: 98.737% (537080/543950)\n","Epoch:  17 \n","Train:  0.9873701626987774\n","Val:  0.9753631821039743\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 326ms | Tot: 1m51s | Loss: 0.0001 | Acc: 98.739% (537091/543950)\n","Epoch:  18 \n","Train:  0.9873903851456935\n","Val:  0.9737747774578908\n"," 1088/1088 [================================================================================>]  Step: 307ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.745% (537125/543950)\n","Epoch:  19 \n","Train:  0.9874528908907069\n","Val:  0.9742380621463318\n"," 1088/1088 [================================================================================>]  Step: 317ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.736% (537076/543950)\n","Epoch:  20 \n","Train:  0.987362809081717\n","Val:  0.9740064198021112\n"," 1088/1088 [================================================================================>]  Step: 337ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.756% (537182/543950)\n","Epoch:  21 \n","Train:  0.9875576799338175\n","Val:  0.9746351633078527\n"," 1088/1088 [================================================================================>]  Step: 311ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.752% (537161/543950)\n","Epoch:  22 \n","Train:  0.9875190734442504\n","Val:  0.9739402362751911\n"," 1088/1088 [================================================================================>]  Step: 329ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.767% (537243/543950)\n","Epoch:  23 \n","Train:  0.9876698225939884\n","Val:  0.9741222409742215\n"," 1088/1088 [================================================================================>]  Step: 306ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.756% (537183/543950)\n","Epoch:  24 \n","Train:  0.9875595183380825\n","Val:  0.9748502597703431\n"," 1088/1088 [================================================================================>]  Step: 305ms | Tot: 1m54s | Loss: 0.0001 | Acc: 98.770% (537257/543950)\n","Epoch:  25 \n","Train:  0.9876955602536998\n","Val:  0.9743869750819021\n"," 1088/1088 [================================================================================>]  Step: 319ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.775% (537285/543950)\n","Epoch:  26 \n","Train:  0.9877470355731225\n","Val:  0.9745358880174725\n"," 1088/1088 [================================================================================>]  Step: 306ms | Tot: 1m52s | Loss: 0.0001 | Acc: 98.773% (537275/543950)\n","Epoch:  27 \n","Train:  0.9877286515304715\n","Val:  0.9742380621463318\n"," 1088/1088 [================================================================================>]  Step: 306ms | Tot: 1m54s | Loss: 0.0000 | Acc: 98.771% (537263/543950)\n","Epoch:  28 \n","Train:  0.9877065906792903\n","Val:  0.9722856481021873\n"," 1088/1088 [================================================================================>]  Step: 317ms | Tot: 1m52s | Loss: 0.0001 | Acc: 98.782% (537326/543950)\n","Epoch:  29 \n","Train:  0.9878224101479915\n","Val:  0.9741884245011416\n"," 1088/1088 [================================================================================>]  Step: 302ms | Tot: 1m54s | Loss: 0.0000 | Acc: 98.785% (537339/543950)\n","Epoch:  30 \n","Train:  0.9878463094034378\n","Val:  0.9740726033290315\n"," 1088/1088 [================================================================================>]  Step: 318ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.837% (537625/543950)\n","Epoch:  1 \n","Train:  0.9883720930232558\n","Val:  0.975644462093385\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 529ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.832% (537596/543950)\n","Epoch:  2 \n","Train:  0.988318779299568\n","Val:  0.9756775538568451\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 320ms | Tot: 1m54s | Loss: 0.0000 | Acc: 98.829% (537578/543950)\n","Epoch:  3 \n","Train:  0.9882856880227963\n","Val:  0.9758099209106853\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 312ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.833% (537603/543950)\n","Epoch:  4 \n","Train:  0.9883316481294236\n","Val:  0.975694099738575\n"," 1088/1088 [================================================================================>]  Step: 312ms | Tot: 1m54s | Loss: 0.0000 | Acc: 98.830% (537587/543950)\n","Epoch:  5 \n","Train:  0.9883022336611821\n","Val:  0.975661007975115\n"," 1088/1088 [================================================================================>]  Step: 302ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.834% (537609/543950)\n","Epoch:  6 \n","Train:  0.9883426785550142\n","Val:  0.975644462093385\n"," 1088/1088 [================================================================================>]  Step: 301ms | Tot: 1m53s | Loss: 0.0001 | Acc: 98.845% (537665/543950)\n","Epoch:  7 \n","Train:  0.9884456291938597\n","Val:  0.9759257420827956\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 297ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.834% (537607/543950)\n","Epoch:  8 \n","Train:  0.988339001746484\n","Val:  0.9756113703299248\n"," 1088/1088 [================================================================================>]  Step: 492ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.835% (537611/543950)\n","Epoch:  9 \n","Train:  0.9883463553635444\n","Val:  0.975694099738575\n"," 1088/1088 [================================================================================>]  Step: 300ms | Tot: 1m51s | Loss: 0.0001 | Acc: 98.846% (537673/543950)\n","Epoch:  10 \n","Train:  0.9884603364279805\n","Val:  0.9758761044376055\n"," 1088/1088 [================================================================================>]  Step: 297ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.841% (537643/543950)\n","Epoch:  11 \n","Train:  0.9884051843000276\n","Val:  0.9756775538568451\n"," 1088/1088 [================================================================================>]  Step: 303ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.840% (537638/543950)\n","Epoch:  12 \n","Train:  0.9883959922787021\n","Val:  0.9757271915020351\n"," 1088/1088 [================================================================================>]  Step: 299ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.850% (537695/543950)\n","Epoch:  13 \n","Train:  0.9885007813218126\n","Val:  0.9757768291472253\n"," 1088/1088 [================================================================================>]  Step: 291ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.845% (537668/543950)\n","Epoch:  14 \n","Train:  0.988451144406655\n","Val:  0.9755286409212747\n"," 1088/1088 [================================================================================>]  Step: 295ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.847% (537679/543950)\n","Epoch:  15 \n","Train:  0.9884713668535711\n","Val:  0.9756775538568451\n"," 1088/1088 [================================================================================>]  Step: 299ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.836% (537619/543950)\n","Epoch:  16 \n","Train:  0.9883610625976652\n","Val:  0.9757106456203051\n"," 1088/1088 [================================================================================>]  Step: 302ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.843% (537657/543950)\n","Epoch:  17 \n","Train:  0.9884309219597389\n","Val:  0.9758099209106853\n"," 1088/1088 [================================================================================>]  Step: 300ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.847% (537678/543950)\n","Epoch:  18 \n","Train:  0.988469528449306\n","Val:  0.975694099738575\n"," 1088/1088 [================================================================================>]  Step: 306ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.846% (537674/543950)\n","Epoch:  19 \n","Train:  0.9884621748322456\n","Val:  0.9756775538568451\n"," 1088/1088 [================================================================================>]  Step: 290ms | Tot: 1m51s | Loss: 0.0001 | Acc: 98.847% (537679/543950)\n","Epoch:  20 \n","Train:  0.9884713668535711\n","Val:  0.9759919256097157\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 293ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.852% (537704/543950)\n","Epoch:  21 \n","Train:  0.9885173269601986\n","Val:  0.9758595585558755\n"," 1088/1088 [================================================================================>]  Step: 297ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.852% (537707/543950)\n","Epoch:  22 \n","Train:  0.9885228421729938\n","Val:  0.975644462093385\n"," 1088/1088 [================================================================================>]  Step: 307ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.849% (537687/543950)\n","Epoch:  23 \n","Train:  0.9884860740876918\n","Val:  0.9756113703299248\n"," 1088/1088 [================================================================================>]  Step: 300ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.856% (537728/543950)\n","Epoch:  24 \n","Train:  0.9885614486625609\n","Val:  0.9758430126741454\n"," 1088/1088 [================================================================================>]  Step: 294ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.850% (537695/543950)\n","Epoch:  25 \n","Train:  0.9885007813218126\n","Val:  0.9753797279857044\n"," 1088/1088 [================================================================================>]  Step: 298ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.855% (537724/543950)\n","Epoch:  26 \n","Train:  0.9885540950455005\n","Val:  0.9756279162116549\n"," 1088/1088 [================================================================================>]  Step: 532ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.862% (537762/543950)\n","Epoch:  27 \n","Train:  0.9886239544075742\n","Val:  0.975661007975115\n"," 1088/1088 [================================================================================>]  Step: 313ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.860% (537749/543950)\n","Epoch:  28 \n","Train:  0.9886000551521279\n","Val:  0.9757106456203051\n"," 1088/1088 [================================================================================>]  Step: 293ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.861% (537752/543950)\n","Epoch:  29 \n","Train:  0.9886055703649232\n","Val:  0.9756279162116549\n"," 1088/1088 [================================================================================>]  Step: 304ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.861% (537753/543950)\n","Epoch:  30 \n","Train:  0.9886074087691883\n","Val:  0.9757602832654952\n"," 1088/1088 [================================================================================>]  Step: 308ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.850% (537692/543950)\n","Epoch:  1 \n","Train:  0.9884952661090174\n","Val:  0.9757602832654952\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 297ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.852% (537707/543950)\n","Epoch:  2 \n","Train:  0.9885228421729938\n","Val:  0.9757933750289552\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 305ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.854% (537717/543950)\n","Epoch:  3 \n","Train:  0.9885412262156448\n","Val:  0.9758099209106853\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 305ms | Tot: 1m53s | Loss: 0.0001 | Acc: 98.854% (537716/543950)\n","Epoch:  4 \n","Train:  0.9885393878113797\n","Val:  0.9757933750289552\n"," 1088/1088 [================================================================================>]  Step: 308ms | Tot: 1m51s | Loss: 0.0001 | Acc: 98.853% (537710/543950)\n","Epoch:  5 \n","Train:  0.9885283573857891\n","Val:  0.9757437373837652\n"," 1088/1088 [================================================================================>]  Step: 297ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.852% (537705/543950)\n","Epoch:  6 \n","Train:  0.9885191653644636\n","Val:  0.9757437373837652\n"," 1088/1088 [================================================================================>]  Step: 310ms | Tot: 1m51s | Loss: 0.0001 | Acc: 98.849% (537690/543950)\n","Epoch:  7 \n","Train:  0.9884915893004872\n","Val:  0.9757768291472253\n"," 1088/1088 [================================================================================>]  Step: 320ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.852% (537703/543950)\n","Epoch:  8 \n","Train:  0.9885154885559334\n","Val:  0.9758430126741454\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 302ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.852% (537708/543950)\n","Epoch:  9 \n","Train:  0.988524680577259\n","Val:  0.9757768291472253\n"," 1088/1088 [================================================================================>]  Step: 306ms | Tot: 1m54s | Loss: 0.0001 | Acc: 98.851% (537702/543950)\n","Epoch:  10 \n","Train:  0.9885136501516684\n","Val:  0.9758430126741454\n"," 1088/1088 [================================================================================>]  Step: 296ms | Tot: 1m52s | Loss: 0.0001 | Acc: 98.855% (537723/543950)\n","Epoch:  11 \n","Train:  0.9885522566412354\n","Val:  0.9757437373837652\n"," 1088/1088 [================================================================================>]  Step: 305ms | Tot: 1m54s | Loss: 0.0001 | Acc: 98.852% (537705/543950)\n","Epoch:  12 \n","Train:  0.9885191653644636\n","Val:  0.9758761044376055\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 293ms | Tot: 1m55s | Loss: 0.0000 | Acc: 98.857% (537733/543950)\n","Epoch:  13 \n","Train:  0.9885706406838863\n","Val:  0.9757768291472253\n"," 1088/1088 [================================================================================>]  Step: 303ms | Tot: 1m53s | Loss: 0.0000 | Acc: 98.856% (537725/543950)\n","Epoch:  14 \n","Train:  0.9885559334497656\n","Val:  0.9757768291472253\n"," 1088/1088 [================================================================================>]  Step: 296ms | Tot: 1m55s | Loss: 0.0000 | Acc: 98.851% (537702/543950)\n","Epoch:  15 \n","Train:  0.9885136501516684\n","Val:  0.9758595585558755\n"," 1088/1088 [================================================================================>]  Step: 299ms | Tot: 1m54s | Loss: 0.0000 | Acc: 98.851% (537702/543950)\n","Epoch:  16 \n","Train:  0.9885136501516684\n","Val:  0.975661007975115\n"," 1088/1088 [================================================================================>]  Step: 298ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.850% (537697/543950)\n","Epoch:  17 \n","Train:  0.9885044581303428\n","Val:  0.9758430126741454\n"," 1088/1088 [================================================================================>]  Step: 319ms | Tot: 1m54s | Loss: 0.0001 | Acc: 98.853% (537709/543950)\n","Epoch:  18 \n","Train:  0.988526518981524\n","Val:  0.9759588338462557\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 299ms | Tot: 1m54s | Loss: 0.0000 | Acc: 98.858% (537740/543950)\n","Epoch:  19 \n","Train:  0.9885835095137421\n","Val:  0.9758264667924154\n"," 1088/1088 [================================================================================>]  Step: 293ms | Tot: 1m52s | Loss: 0.0001 | Acc: 98.854% (537715/543950)\n","Epoch:  20 \n","Train:  0.9885375494071146\n","Val:  0.9757602832654952\n"," 1088/1088 [================================================================================>]  Step: 344ms | Tot: 1m54s | Loss: 0.0000 | Acc: 98.857% (537733/543950)\n","Epoch:  21 \n","Train:  0.9885706406838863\n","Val:  0.9758430126741454\n"," 1088/1088 [================================================================================>]  Step: 295ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.853% (537710/543950)\n","Epoch:  22 \n","Train:  0.9885283573857891\n","Val:  0.9758595585558755\n"," 1088/1088 [================================================================================>]  Step: 318ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.851% (537699/543950)\n","Epoch:  23 \n","Train:  0.988508134938873\n","Val:  0.9757933750289552\n"," 1088/1088 [================================================================================>]  Step: 307ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.850% (537694/543950)\n","Epoch:  24 \n","Train:  0.9884989429175476\n","Val:  0.975694099738575\n"," 1088/1088 [================================================================================>]  Step: 306ms | Tot: 1m51s | Loss: 0.0000 | Acc: 98.852% (537704/543950)\n","Epoch:  25 \n","Train:  0.9885173269601986\n","Val:  0.9757602832654952\n"," 1088/1088 [================================================================================>]  Step: 308ms | Tot: 1m55s | Loss: 0.0000 | Acc: 98.851% (537701/543950)\n","Epoch:  26 \n","Train:  0.9885118117474032\n","Val:  0.9757768291472253\n"," 1088/1088 [================================================================================>]  Step: 299ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.851% (537700/543950)\n","Epoch:  27 \n","Train:  0.9885099733431382\n","Val:  0.9758264667924154\n"," 1088/1088 [================================================================================>]  Step: 291ms | Tot: 1m55s | Loss: 0.0000 | Acc: 98.854% (537718/543950)\n","Epoch:  28 \n","Train:  0.9885430646199099\n","Val:  0.9758264667924154\n"," 1088/1088 [================================================================================>]  Step: 297ms | Tot: 1m52s | Loss: 0.0000 | Acc: 98.855% (537722/543950)\n","Epoch:  29 \n","Train:  0.9885504182369703\n","Val:  0.9757933750289552\n"," 1088/1088 [================================================================================>]  Step: 300ms | Tot: 1m53s | Loss: 0.0001 | Acc: 98.853% (537712/543950)\n","Epoch:  30 \n","Train:  0.9885320341943193\n","Val:  0.9758430126741454\n","svhn_root.py:140: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  outputs = m(outputs)\n","Traceback (most recent call last):\n","  File \"svhn_root.py\", line 195, in <module>\n","    main()\n","  File \"svhn_root.py\", line 192, in main\n","    average_softmax(model, train_loader, val_loader, torch.device(\"cuda\"))\n","  File \"svhn_root.py\", line 144, in average_softmax\n","    soft_out[categ] += hold.sum(dim=0)\n","RuntimeError: The size of tensor a (10) must match the size of tensor b (6) at non-singleton dimension 1\n"]}]},{"cell_type":"code","source":["!python svhn_1.py"],"metadata":{"id":"XfOQGwUs2x5c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673954512493,"user_tz":-480,"elapsed":4993392,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"9bd3f6e9-eabe-4bac-a6bf-9733fb69013c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using downloaded and verified file: ../dataSVHN/extra_32x32.mat\n","tcmalloc: large alloc 1631641600 bytes == 0x787a000 @  0x7fe5bfd651e7 0x4d30a0 0x5dede2 0x7fe537f3119a 0x7fe537f50be6 0x7fe537f5c159 0x7fe537f525af 0x7fe537f5a9fb 0x7fe537f506d1 0x4f9336 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997c7 0x55d078 0x5d8941 0x5da107 0x586de6 0x5d8cdf 0x55ea20 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630\n","Using downloaded and verified file: ../dataSVHN/train_32x32.mat\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 32, 32]             448\n","       BatchNorm2d-2           [-1, 16, 32, 32]              32\n","              ReLU-3           [-1, 16, 32, 32]               0\n","            Conv2d-4           [-1, 32, 32, 32]           4,640\n","       BatchNorm2d-5           [-1, 32, 32, 32]              64\n","              ReLU-6           [-1, 32, 32, 32]               0\n","            Conv2d-7           [-1, 32, 32, 32]           9,248\n","       BatchNorm2d-8           [-1, 32, 32, 32]              64\n","              ReLU-9           [-1, 32, 32, 32]               0\n","        MaxPool2d-10           [-1, 32, 16, 16]               0\n","        AvgPool2d-11           [-1, 32, 16, 16]               0\n","           Linear-12                    [-1, 6]          49,158\n","================================================================\n","Total params: 63,654\n","Trainable params: 63,654\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 2.00\n","Params size (MB): 0.24\n","Estimated Total Size (MB): 2.25\n","----------------------------------------------------------------\n","None\n"," 1088/1088 [================================================================================>]  Step: 1s516ms | Tot: 1m59s | Loss: 0.0001 | Acc: 99.207% (137495/138594)\n","Epoch:  1 \n","Train:  0.9920703637964126\n","Val:  0.9876126856475776\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 1s14ms | Tot: 1m55s | Loss: 0.0000 | Acc: 99.214% (137504/138594)\n","Epoch:  2 \n","Train:  0.9921353016725111\n","Val:  0.9900771775082691\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 832ms | Tot: 1m55s | Loss: 0.0000 | Acc: 99.288% (137607/138594)\n","Epoch:  3 \n","Train:  0.9928784795878609\n","Val:  0.9883260911861989\n"," 1088/1088 [================================================================================>]  Step: 676ms | Tot: 1m56s | Loss: 0.0000 | Acc: 99.244% (137546/138594)\n","Epoch:  4 \n","Train:  0.9924383450943042\n","Val:  0.9900123224593035\n"," 1088/1088 [================================================================================>]  Step: 555ms | Tot: 1m56s | Loss: 0.0000 | Acc: 99.218% (137510/138594)\n","Epoch:  5 \n","Train:  0.9921785935899101\n","Val:  0.9905311628510279\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 459ms | Tot: 1m58s | Loss: 0.0000 | Acc: 99.254% (137560/138594)\n","Epoch:  6 \n","Train:  0.9925393595682352\n","Val:  0.9871587003048188\n"," 1088/1088 [================================================================================>]  Step: 427ms | Tot: 1m54s | Loss: 0.0000 | Acc: 99.278% (137594/138594)\n","Epoch:  7 \n","Train:  0.9927846804334964\n","Val:  0.98903949672482\n"," 1088/1088 [================================================================================>]  Step: 389ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.242% (137544/138594)\n","Epoch:  8 \n","Train:  0.9924239144551712\n","Val:  0.9893637719696479\n"," 1088/1088 [================================================================================>]  Step: 368ms | Tot: 1m56s | Loss: 0.0000 | Acc: 99.217% (137509/138594)\n","Epoch:  9 \n","Train:  0.9921713782703436\n","Val:  0.9899474674103379\n"," 1088/1088 [================================================================================>]  Step: 373ms | Tot: 1m54s | Loss: 0.0000 | Acc: 99.259% (137567/138594)\n","Epoch:  10 \n","Train:  0.9925898668052008\n","Val:  0.9910500032427525\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 371ms | Tot: 1m55s | Loss: 0.0000 | Acc: 99.252% (137557/138594)\n","Epoch:  11 \n","Train:  0.9925177136095358\n","Val:  0.9907905830468902\n"," 1088/1088 [================================================================================>]  Step: 375ms | Tot: 1m54s | Loss: 0.0000 | Acc: 99.257% (137564/138594)\n","Epoch:  12 \n","Train:  0.9925682208465013\n","Val:  0.9905311628510279\n"," 1088/1088 [================================================================================>]  Step: 375ms | Tot: 1m55s | Loss: 0.0000 | Acc: 99.236% (137535/138594)\n","Epoch:  13 \n","Train:  0.9923589765790727\n","Val:  0.9880666709903366\n"," 1088/1088 [================================================================================>]  Step: 355ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.279% (137595/138594)\n","Epoch:  14 \n","Train:  0.9927918957530629\n","Val:  0.9854724690317141\n"," 1088/1088 [================================================================================>]  Step: 368ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.244% (137546/138594)\n","Epoch:  15 \n","Train:  0.9924383450943042\n","Val:  0.9883909462351644\n"," 1088/1088 [================================================================================>]  Step: 353ms | Tot: 1m55s | Loss: 0.0000 | Acc: 99.261% (137570/138594)\n","Epoch:  16 \n","Train:  0.9926115127639004\n","Val:  0.9902068876062001\n"," 1088/1088 [================================================================================>]  Step: 356ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.265% (137576/138594)\n","Epoch:  17 \n","Train:  0.9926548046812993\n","Val:  0.9902068876062001\n"," 1088/1088 [================================================================================>]  Step: 349ms | Tot: 1m54s | Loss: 0.0000 | Acc: 99.254% (137560/138594)\n","Epoch:  18 \n","Train:  0.9925393595682352\n","Val:  0.9912445683896491\n","checkpoint saved\n"," 1088/1088 [================================================================================>]  Step: 348ms | Tot: 1m55s | Loss: 0.0000 | Acc: 99.245% (137547/138594)\n","Epoch:  19 \n","Train:  0.9924455604138708\n","Val:  0.9899474674103379\n"," 1088/1088 [================================================================================>]  Step: 352ms | Tot: 1m53s | Loss: 0.0001 | Acc: 99.265% (137576/138594)\n","Epoch:  20 \n","Train:  0.9926548046812993\n","Val:  0.9898177573124067\n"," 1088/1088 [================================================================================>]  Step: 348ms | Tot: 1m53s | Loss: 0.0001 | Acc: 99.255% (137561/138594)\n","Epoch:  21 \n","Train:  0.9925465748878017\n","Val:  0.9897529022634413\n"," 1088/1088 [================================================================================>]  Step: 344ms | Tot: 1m52s | Loss: 0.0000 | Acc: 99.260% (137569/138594)\n","Epoch:  22 \n","Train:  0.9926042974443338\n","Val:  0.987418120500681\n"," 1088/1088 [================================================================================>]  Step: 353ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.261% (137570/138594)\n","Epoch:  23 \n","Train:  0.9926115127639004\n","Val:  0.9895583371165445\n"," 1088/1088 [================================================================================>]  Step: 349ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.259% (137567/138594)\n","Epoch:  24 \n","Train:  0.9925898668052008\n","Val:  0.9900771775082691\n"," 1088/1088 [================================================================================>]  Step: 339ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.289% (137608/138594)\n","Epoch:  25 \n","Train:  0.9928856949074274\n","Val:  0.9899474674103379\n"," 1088/1088 [================================================================================>]  Step: 341ms | Tot: 1m53s | Loss: 0.0001 | Acc: 99.252% (137557/138594)\n","Epoch:  26 \n","Train:  0.9925177136095358\n","Val:  0.9902068876062001\n"," 1088/1088 [================================================================================>]  Step: 349ms | Tot: 1m52s | Loss: 0.0001 | Acc: 99.293% (137614/138594)\n","Epoch:  27 \n","Train:  0.9929289868248264\n","Val:  0.9903365977041313\n"," 1088/1088 [================================================================================>]  Step: 340ms | Tot: 1m55s | Loss: 0.0000 | Acc: 99.269% (137581/138594)\n","Epoch:  28 \n","Train:  0.9926908812791319\n","Val:  0.9889097866268889\n"," 1088/1088 [================================================================================>]  Step: 335ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.265% (137576/138594)\n","Epoch:  29 \n","Train:  0.9926548046812993\n","Val:  0.9892340618717167\n"," 1088/1088 [================================================================================>]  Step: 336ms | Tot: 1m54s | Loss: 0.0000 | Acc: 99.269% (137581/138594)\n","Epoch:  30 \n","Train:  0.9926908812791319\n","Val:  0.9900123224593035\n"," 1088/1088 [================================================================================>]  Step: 336ms | Tot: 1m54s | Loss: 0.0000 | Acc: 99.745% (138240/138594)\n","Epoch:  1 \n","Train:  0.9974457768734577\n","Val:  0.9901420325572345\n"," 1088/1088 [================================================================================>]  Step: 319ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.749% (138246/138594)\n","Epoch:  2 \n","Train:  0.9974890687908567\n","Val:  0.9900123224593035\n"," 1088/1088 [================================================================================>]  Step: 321ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.753% (138252/138594)\n","Epoch:  3 \n","Train:  0.9975323607082558\n","Val:  0.9892340618717167\n"," 1088/1088 [================================================================================>]  Step: 313ms | Tot: 1m52s | Loss: 0.0000 | Acc: 99.730% (138220/138594)\n","Epoch:  4 \n","Train:  0.9973014704821277\n","Val:  0.9893637719696479\n"," 1088/1088 [================================================================================>]  Step: 318ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.749% (138246/138594)\n","Epoch:  5 \n","Train:  0.9974890687908567\n","Val:  0.9895583371165445\n"," 1088/1088 [================================================================================>]  Step: 321ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.753% (138251/138594)\n","Epoch:  6 \n","Train:  0.9975251453886893\n","Val:  0.9900771775082691\n"," 1088/1088 [================================================================================>]  Step: 318ms | Tot: 1m52s | Loss: 0.0000 | Acc: 99.757% (138257/138594)\n","Epoch:  7 \n","Train:  0.9975684373060882\n","Val:  0.9894286270186134\n"," 1088/1088 [================================================================================>]  Step: 316ms | Tot: 1m53s | Loss: 0.0000 | Acc: 99.763% (138266/138594)\n","Epoch:  8 \n","Train:  0.9976333751821869\n","Val:  0.9901420325572345\n"," 1088/1088 [================================================================================>]  Step: 319ms | Tot: 1m52s | Loss: 0.0000 | Acc: 99.751% (138249/138594)\n","Epoch:  9 \n","Train:  0.9975107147495562\n","Val:  0.9900771775082691\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 628, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 671, in _next_data\n","    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataset.py\", line 240, in __getitem__\n","    return self.datasets[dataset_idx][sample_idx]\n","  File \"/usr/local/lib/python3.8/dist-packages/torchvision/datasets/svhn.py\", line 108, in __getitem__\n","    img = self.transform(img)\n","  File \"/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py\", line 135, in __call__\n","    return F.to_tensor(pic)\n","  File \"/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py\", line 167, in to_tensor\n","    img = img.view(pic.size[1], pic.size[0], len(pic.getbands()))\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"svhn_1.py\", line 270, in <module>\n","    main()\n","  File \"svhn_1.py\", line 264, in main\n","    max = train(model, model_one, optimizer, loss_fn, train_loader, val_loader, torch.device(\"cuda\"), max)\n","  File \"svhn_1.py\", line 117, in train\n","    for batch_num, (data, target) in enumerate(trainloader):\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 641, in __next__\n","    return data\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/profiler.py\", line 493, in __exit__\n","    torch.ops.profiler._record_function_exit(self.handle)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/_ops.py\", line 442, in __call__\n","    return self._op(*args, **kwargs or {})\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"code","source":["!python svhn_3.py"],"metadata":{"id":"MlJwZ4mL22EC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673962742599,"user_tz":-480,"elapsed":8223660,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"7ff44884-d5fa-451d-dad5-86d33e2d67ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using downloaded and verified file: ../dataSVHN/extra_32x32.mat\n","tcmalloc: large alloc 1631641600 bytes == 0x7b0a000 @  0x7fc742dc81e7 0x4d30a0 0x5dede2 0x7fc6baf9419a 0x7fc6bafb3be6 0x7fc6bafbf159 0x7fc6bafb55af 0x7fc6bafbd9fb 0x7fc6bafb36d1 0x4f9336 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997c7 0x55d078 0x5d8941 0x5da107 0x586de6 0x5d8cdf 0x55ea20 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630\n","Using downloaded and verified file: ../dataSVHN/train_32x32.mat\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 32, 32]             448\n","       BatchNorm2d-2           [-1, 16, 32, 32]              32\n","              ReLU-3           [-1, 16, 32, 32]               0\n","            Conv2d-4           [-1, 32, 32, 32]           4,640\n","       BatchNorm2d-5           [-1, 32, 32, 32]              64\n","              ReLU-6           [-1, 32, 32, 32]               0\n","            Conv2d-7           [-1, 32, 32, 32]           9,248\n","       BatchNorm2d-8           [-1, 32, 32, 32]              64\n","              ReLU-9           [-1, 32, 32, 32]               0\n","        MaxPool2d-10           [-1, 32, 16, 16]               0\n","        AvgPool2d-11           [-1, 32, 16, 16]               0\n","           Linear-12                    [-1, 6]          49,158\n","================================================================\n","Total params: 63,654\n","Trainable params: 63,654\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 2.00\n","Params size (MB): 0.24\n","Estimated Total Size (MB): 2.25\n","----------------------------------------------------------------\n","None\n"," 1088/1088 [================================================================================>]  Step: 1s28ms | Tot: 1m57s | Loss: 0.0001 | Acc: 98.204% (192048/195561)\n","Epoch:  1 \n","Train:  0.9820362955804072\n","checkpoint saved\n","Val:  0.979114069313748\n"," 1088/1088 [================================================================================>]  Step: 914ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.285% (192207/195561)\n","Epoch:  2 \n","Train:  0.9828493411262982\n","checkpoint saved\n","Val:  0.9762680743630938\n"," 1088/1088 [================================================================================>]  Step: 697ms | Tot: 1m58s | Loss: 0.0000 | Acc: 98.255% (192148/195561)\n","Epoch:  3 \n","Train:  0.9825476449803386\n","Val:  0.9639660316731696\n"," 1088/1088 [================================================================================>]  Step: 604ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.297% (192230/195561)\n","Epoch:  4 \n","Train:  0.9829669514882824\n","checkpoint saved\n","Val:  0.9810420013770943\n"," 1088/1088 [================================================================================>]  Step: 514ms | Tot: 1m58s | Loss: 0.0000 | Acc: 98.279% (192196/195561)\n","Epoch:  5 \n","Train:  0.9827930926923057\n","Val:  0.980169841634152\n"," 1088/1088 [================================================================================>]  Step: 447ms | Tot: 1m58s | Loss: 0.0001 | Acc: 98.307% (192250/195561)\n","Epoch:  6 \n","Train:  0.9830692213682687\n","checkpoint saved\n","Val:  0.9809501950883636\n"," 1088/1088 [================================================================================>]  Step: 396ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.305% (192246/195561)\n","Epoch:  7 \n","Train:  0.9830487673922714\n","Val:  0.9785632315813633\n"," 1088/1088 [================================================================================>]  Step: 374ms | Tot: 1m58s | Loss: 0.0001 | Acc: 98.256% (192150/195561)\n","Epoch:  8 \n","Train:  0.9825578719683372\n","Val:  0.9815010328207482\n"," 1088/1088 [================================================================================>]  Step: 347ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.274% (192186/195561)\n","Epoch:  9 \n","Train:  0.9827419577523125\n","Val:  0.9785173284369979\n"," 1088/1088 [================================================================================>]  Step: 350ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.275% (192187/195561)\n","Epoch:  10 \n","Train:  0.9827470712463119\n","Val:  0.9781042001377094\n"," 1088/1088 [================================================================================>]  Step: 334ms | Tot: 1m58s | Loss: 0.0001 | Acc: 98.300% (192237/195561)\n","Epoch:  11 \n","Train:  0.9830027459462777\n","Val:  0.9750286894652284\n"," 1088/1088 [================================================================================>]  Step: 330ms | Tot: 1m57s | Loss: 0.0001 | Acc: 98.271% (192179/195561)\n","Epoch:  12 \n","Train:  0.9827061632943174\n","Val:  0.9804911636447097\n"," 1088/1088 [================================================================================>]  Step: 323ms | Tot: 1m58s | Loss: 0.0001 | Acc: 98.268% (192173/195561)\n","Epoch:  13 \n","Train:  0.9826754823303215\n","Val:  0.9788386504475557\n"," 1088/1088 [================================================================================>]  Step: 321ms | Tot: 1m58s | Loss: 0.0001 | Acc: 98.303% (192243/195561)\n","Epoch:  14 \n","Train:  0.9830334269102735\n","Val:  0.9811338076658251\n"," 1088/1088 [================================================================================>]  Step: 315ms | Tot: 1m57s | Loss: 0.0001 | Acc: 98.304% (192244/195561)\n","Epoch:  15 \n","Train:  0.9830385404042729\n","Val:  0.9797567133348635\n"," 1088/1088 [================================================================================>]  Step: 315ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.274% (192185/195561)\n","Epoch:  16 \n","Train:  0.9827368442583133\n","Val:  0.9803075510672481\n"," 1088/1088 [================================================================================>]  Step: 311ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.294% (192225/195561)\n","Epoch:  17 \n","Train:  0.9829413840182859\n","Val:  0.9792517787468441\n"," 1088/1088 [================================================================================>]  Step: 313ms | Tot: 1m57s | Loss: 0.0001 | Acc: 98.290% (192217/195561)\n","Epoch:  18 \n","Train:  0.9829004760662914\n","Val:  0.9766812026623823\n"," 1088/1088 [================================================================================>]  Step: 323ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.298% (192233/195561)\n","Epoch:  19 \n","Train:  0.9829822919702804\n","Val:  0.9716777599265549\n"," 1088/1088 [================================================================================>]  Step: 313ms | Tot: 1m57s | Loss: 0.0001 | Acc: 98.309% (192255/195561)\n","Epoch:  20 \n","Train:  0.9830947888382653\n","checkpoint saved\n","Val:  0.979114069313748\n"," 1088/1088 [================================================================================>]  Step: 313ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.311% (192257/195561)\n","Epoch:  21 \n","Train:  0.983105015826264\n","checkpoint saved\n","Val:  0.9802616479228827\n"," 1088/1088 [================================================================================>]  Step: 323ms | Tot: 1m57s | Loss: 0.0001 | Acc: 98.339% (192312/195561)\n","Epoch:  22 \n","Train:  0.9833862579962263\n","checkpoint saved\n","Val:  0.9803075510672481\n"," 1088/1088 [================================================================================>]  Step: 308ms | Tot: 1m56s | Loss: 0.0000 | Acc: 98.300% (192237/195561)\n","Epoch:  23 \n","Train:  0.9830027459462777\n","Val:  0.9800780353454212\n"," 1088/1088 [================================================================================>]  Step: 315ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.319% (192273/195561)\n","Epoch:  24 \n","Train:  0.983186831730253\n","Val:  0.9789304567362864\n"," 1088/1088 [================================================================================>]  Step: 315ms | Tot: 1m56s | Loss: 0.0000 | Acc: 98.327% (192290/195561)\n","Epoch:  25 \n","Train:  0.9832737611282413\n","Val:  0.9800321322010558\n"," 1088/1088 [================================================================================>]  Step: 308ms | Tot: 1m57s | Loss: 0.0001 | Acc: 98.303% (192242/195561)\n","Epoch:  26 \n","Train:  0.9830283134162742\n","Val:  0.9792517787468441\n"," 1088/1088 [================================================================================>]  Step: 309ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.300% (192237/195561)\n","Epoch:  27 \n","Train:  0.9830027459462777\n","Val:  0.9788845535919211\n"," 1088/1088 [================================================================================>]  Step: 310ms | Tot: 1m56s | Loss: 0.0001 | Acc: 98.285% (192207/195561)\n","Epoch:  28 \n","Train:  0.9828493411262982\n","Val:  0.9793894881799403\n"," 1088/1088 [================================================================================>]  Step: 315ms | Tot: 1m57s | Loss: 0.0000 | Acc: 98.306% (192249/195561)\n","Epoch:  29 \n","Train:  0.9830641078742695\n","Val:  0.9802157447785174\n"," 1088/1088 [================================================================================>]  Step: 304ms | Tot: 1m56s | Loss: 0.0000 | Acc: 98.304% (192244/195561)\n","Epoch:  30 \n","Train:  0.9830385404042729\n","Val:  0.980169841634152\n"," 1088/1088 [================================================================================>]  Step: 295ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.158% (193914/195561)\n","Epoch:  1 \n","Train:  0.9915780753831286\n","checkpoint saved\n","Val:  0.9825109019967868\n"," 1088/1088 [================================================================================>]  Step: 298ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.173% (193944/195561)\n","Epoch:  2 \n","Train:  0.991731480203108\n","checkpoint saved\n","Val:  0.9827863208629791\n"," 1088/1088 [================================================================================>]  Step: 295ms | Tot: 1m56s | Loss: 0.0000 | Acc: 99.217% (194030/195561)\n","Epoch:  3 \n","Train:  0.9921712406870491\n","checkpoint saved\n","Val:  0.9829240302960753\n"," 1088/1088 [================================================================================>]  Step: 303ms | Tot: 1m58s | Loss: 0.0000 | Acc: 99.185% (193967/195561)\n","Epoch:  4 \n","Train:  0.9918490905650922\n","Val:  0.9820059674087676\n"," 1088/1088 [================================================================================>]  Step: 303ms | Tot: 1m56s | Loss: 0.0001 | Acc: 99.179% (193955/195561)\n","Epoch:  5 \n","Train:  0.9917877286371004\n","Val:  0.9829699334404407\n"," 1088/1088 [================================================================================>]  Step: 304ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.199% (193994/195561)\n","Epoch:  6 \n","Train:  0.9919871549030738\n","Val:  0.9825568051411522\n"," 1088/1088 [================================================================================>]  Step: 299ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.213% (194022/195561)\n","Epoch:  7 \n","Train:  0.9921303327350546\n","Val:  0.9828781271517099\n"," 1088/1088 [================================================================================>]  Step: 304ms | Tot: 1m56s | Loss: 0.0001 | Acc: 99.213% (194022/195561)\n","Epoch:  8 \n","Train:  0.9921303327350546\n","Val:  0.9829699334404407\n"," 1088/1088 [================================================================================>]  Step: 298ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.228% (194051/195561)\n","Epoch:  9 \n","Train:  0.9922786240610346\n","checkpoint saved\n","Val:  0.9826027082855175\n"," 1088/1088 [================================================================================>]  Step: 296ms | Tot: 1m56s | Loss: 0.0000 | Acc: 99.191% (193979/195561)\n","Epoch:  10 \n","Train:  0.991910452493084\n","Val:  0.9826486114298829\n"," 1088/1088 [================================================================================>]  Step: 298ms | Tot: 1m57s | Loss: 0.0001 | Acc: 99.221% (194037/195561)\n","Epoch:  11 \n","Train:  0.9922070351450443\n","Val:  0.9828322240073445\n"," 1088/1088 [================================================================================>]  Step: 465ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.217% (194030/195561)\n","Epoch:  12 \n","Train:  0.9921712406870491\n","Val:  0.9830158365848061\n"," 1088/1088 [================================================================================>]  Step: 300ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.212% (194020/195561)\n","Epoch:  13 \n","Train:  0.9921201057470559\n","Val:  0.9823731925636906\n"," 1088/1088 [================================================================================>]  Step: 293ms | Tot: 1m58s | Loss: 0.0000 | Acc: 99.235% (194064/195561)\n","Epoch:  14 \n","Train:  0.9923450994830257\n","checkpoint saved\n","Val:  0.982419095708056\n"," 1088/1088 [================================================================================>]  Step: 301ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.213% (194022/195561)\n","Epoch:  15 \n","Train:  0.9921303327350546\n","Val:  0.9815469359651137\n"," 1088/1088 [================================================================================>]  Step: 292ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.225% (194045/195561)\n","Epoch:  16 \n","Train:  0.9922479430970388\n","Val:  0.9826486114298829\n"," 1088/1088 [================================================================================>]  Step: 293ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.227% (194050/195561)\n","Epoch:  17 \n","Train:  0.9922735105670354\n","Val:  0.9831535460179023\n"," 1088/1088 [================================================================================>]  Step: 300ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.265% (194123/195561)\n","Epoch:  18 \n","Train:  0.9926467956289853\n","checkpoint saved\n","Val:  0.9825568051411522\n"," 1088/1088 [================================================================================>]  Step: 294ms | Tot: 1m57s | Loss: 0.0001 | Acc: 99.232% (194059/195561)\n","Epoch:  19 \n","Train:  0.9923195320130291\n","Val:  0.9829699334404407\n"," 1088/1088 [================================================================================>]  Step: 287ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.213% (194021/195561)\n","Epoch:  20 \n","Train:  0.9921252192410552\n","Val:  0.9823731925636906\n"," 1088/1088 [================================================================================>]  Step: 300ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.241% (194077/195561)\n","Epoch:  21 \n","Train:  0.9924115749050169\n","Val:  0.9822813862749599\n"," 1088/1088 [================================================================================>]  Step: 284ms | Tot: 1m56s | Loss: 0.0000 | Acc: 99.244% (194082/195561)\n","Epoch:  22 \n","Train:  0.9924371423750135\n","Val:  0.9828781271517099\n"," 1088/1088 [================================================================================>]  Step: 286ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.214% (194023/195561)\n","Epoch:  23 \n","Train:  0.9921354462290538\n","Val:  0.982419095708056\n"," 1088/1088 [================================================================================>]  Step: 289ms | Tot: 1m56s | Loss: 0.0000 | Acc: 99.256% (194106/195561)\n","Epoch:  24 \n","Train:  0.992559866230997\n","Val:  0.9828781271517099\n"," 1088/1088 [================================================================================>]  Step: 283ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.281% (194155/195561)\n","Epoch:  25 \n","Train:  0.9928104274369633\n","checkpoint saved\n","Val:  0.9831076428735368\n"," 1088/1088 [================================================================================>]  Step: 283ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.267% (194128/195561)\n","Epoch:  26 \n","Train:  0.9926723630989819\n","Val:  0.9826945145742484\n"," 1088/1088 [================================================================================>]  Step: 300ms | Tot: 1m56s | Loss: 0.0000 | Acc: 99.228% (194051/195561)\n","Epoch:  27 \n","Train:  0.9922786240610346\n","Val:  0.982189579986229\n"," 1088/1088 [================================================================================>]  Step: 287ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.228% (194051/195561)\n","Epoch:  28 \n","Train:  0.9922786240610346\n","Val:  0.9810879045214598\n"," 1088/1088 [================================================================================>]  Step: 289ms | Tot: 1m56s | Loss: 0.0000 | Acc: 99.238% (194071/195561)\n","Epoch:  29 \n","Train:  0.9923808939410209\n","Val:  0.9822813862749599\n"," 1088/1088 [================================================================================>]  Step: 290ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.247% (194088/195561)\n","Epoch:  30 \n","Train:  0.9924678233390093\n","Val:  0.9826486114298829\n"," 1088/1088 [================================================================================>]  Step: 288ms | Tot: 1m55s | Loss: 0.0000 | Acc: 99.322% (194236/195561)\n","Epoch:  1 \n","Train:  0.9932246204509079\n","checkpoint saved\n","Val:  0.9831994491622676\n"," 1088/1088 [================================================================================>]  Step: 294ms | Tot: 1m56s | Loss: 0.0000 | Acc: 99.323% (194238/195561)\n","Epoch:  2 \n","Train:  0.9932348474389066\n","checkpoint saved\n","Val:  0.98347486802846\n"," 1088/1088 [================================================================================>]  Step: 298ms | Tot: 1m55s | Loss: 0.0000 | Acc: 99.320% (194232/195561)\n","Epoch:  3 \n","Train:  0.9932041664749106\n","Val:  0.9829699334404407\n"," 1088/1088 [================================================================================>]  Step: 288ms | Tot: 1m57s | Loss: 0.0000 | Acc: 99.323% (194237/195561)\n","Epoch:  4 \n","Train:  0.9932297339449072\n","Val:  0.983245352306633\n","Traceback (most recent call last):\n","  File \"svhn_3.py\", line 270, in <module>\n","    main()\n","  File \"svhn_3.py\", line 264, in main\n","    max = train(model, model_three, optimizer, loss_fn, train_loader, val_loader, torch.device(\"cuda\"), max)\n","  File \"svhn_3.py\", line 145, in train\n","    loss = train_mod(model_three, optimizer, loss_fn, three_data, three_target1, device)\n","  File \"svhn_3.py\", line 101, in train_mod\n","    model.acc += pred.eq(target.view_as(pred)).sum().item()\n","KeyboardInterrupt\n","^C\n"]}]},{"cell_type":"markdown","source":["##3.2 Run the Inference"],"metadata":{"id":"9vYviuYbPXDp"}},{"cell_type":"code","source":["%cd /content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/SVHN/Test/\n","!python test_svhn.py"],"metadata":{"id":"VR_iYQ8pPZHY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1673962793849,"user_tz":-480,"elapsed":39944,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"73aa8748-e84b-4967-f905-af4bb587344a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/SVHN/Test\n","Using downloaded and verified file: ../dataSVHN/extra_32x32.mat\n","tcmalloc: large alloc 1631641600 bytes == 0x6288000 @  0x7f2fce3951e7 0x4d30a0 0x5dede2 0x7f2f4656119a 0x7f2f46580be6 0x7f2f4658c159 0x7f2f465825af 0x7f2f4658a9fb 0x7f2f465806d1 0x4f9336 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997c7 0x55d078 0x5d8941 0x5da107 0x586de6 0x5d8cdf 0x55ea20 0x5d8868 0x4990ca 0x55cd91 0x55d743 0x642630\n","Using downloaded and verified file: ../dataSVHN/train_32x32.mat\n","Val:  0.9829081041728713\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yrCu0ohxa30c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3.3 Demo"],"metadata":{"id":"CiWsRRSmPZZo"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torchvision import models\n","from torch.autograd import Variable\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import time\n","import numpy as np\n","import shutil\n","import os\n","import argparse\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import glob\n","import os, random"],"metadata":{"id":"fJSMd5tJPb6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["m = nn.Softmax()\n","\n","cfg = {\n","    'root': [16, 32, 32,'M'],\n","    '2': [16, 'M', 32, 'M', 'D'],\n","    '3': [32, 64, 'M', 'D'],\n","    '1': [32, 32, 'M', 'D'],\n","    '5': [16, 32, 'M', 32, 32, 'M', 64,'D'],\n","    '6': [16, 32, 32, 'M', 64, 64, 128, 'M', 'D'],\n","}"],"metadata":{"id":"8MJAsDVGPq_P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class model(nn.Module):\n","    def __init__(self, size):\n","        super(model, self).__init__()\n","        self.features = self._make_layers(cfg[size])\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*16*16, 6),\n","                )\n","\n","    def forward(self, x):\n","        y = self.features(x)\n","        x = y.view(y.size(0), -1)\n","        out = self.classifier(x)\n","        return y,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_root():\n","    return model('root')\n","\n","class mod_one(nn.Module):\n","    def __init__(self, size):\n","        super(mod_one, self).__init__()\n","        self.features = self._make_layers(cfg[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(32*8*8, 2),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_1():\n","    return mod_one('1')\n","\n","class mod_three(nn.Module):\n","    def __init__(self, size):\n","        super(mod_three, self).__init__()\n","        self.features = self._make_layers(cfg[size], 32)\n","        self.classifier = nn.Sequential(\n","                        nn.Linear(64*8*8, 4),\n","                )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        out = self.classifier(x)\n","        return x,out\n","\n","    def _make_layers(self, cfg, channels = 3):\n","        layers = []\n","        in_channels = channels\n","        for x in cfg:\n","            if x == 'D':\n","                layers += [nn.Dropout()]\n","            elif x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1), nn.BatchNorm2d(x), nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","def model_3():\n","    return mod_three('3')"],"metadata":{"id":"p6pc_4QjPs2A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["leaves = {\"root\": [0,2,4,9], \"root_3\":[0,1,2,3], \"root_1\": [0,1]}\n","mapper = {\"root\": {0:0,2:2,4:4,9:9}, \"root_3\":{0:3,1:5,2:8,3:6}, \"root_1\": {0:1,1:7}}\n","def is_leaf(path, output):\n","    if output in leaves[path]:\n","        return True\n","    return False\n","\n","def find_category(path, leaf):\n","    return mapper[path][leaf]"],"metadata":{"id":"6XpNSfaOPxIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = {\"root_3\": model_3(), \"root_1\": model_1()}\n","model_name = {\"root_3\": \"../Models/svhn_3.pth\", \"root_1\": \"../Models/svhn_1.pth\"}\n","\n","def fetch_model(path):\n","    model = models[path].cuda().eval()\n","    model.load_state_dict(torch.load(model_name[path]))\n","    return model\n","    "],"metadata":{"id":"f5thR0WwPy4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def demo(model, img, device):\n","    parent = \"root\"\n","    path = \"root\"\n","    next_data, root_out = model(img.view(1,img.shape[0], img.shape[1], img.shape[2]))\n","    root_out = root_out.max(1, keepdim=True)[1]\n","    if is_leaf(path, root_out.item()):\n","            return find_category(path, root_out.item())\n","    parent = str(root_out.item())\n","    \n","    while(1): \n","        path = path + \"_\" + parent\n","        model = fetch_model(path)\n","        model = model.eval()\n","        next_data, model_out = model(next_data)\n","        model_out = model_out.max(1, keepdim=True)[1]\n","        if is_leaf(path, model_out.item()):\n","            return find_category(path, model_out.item())\n","        parent = str(model_out.item())"],"metadata":{"id":"RfZUPzu9P0OZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    img_path = glob.glob('/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/SVHN/Demo/img/svhn/*.jpg')[random.randint(1,len(glob.glob('/content/gdrive/MyDrive/ACLab/006 Project/MNN_Tree/SVHN/Demo/img/svhn/*.jpg'))-1)]\n","    img = Image.open(img_path)\n","    plt.imshow(np.asarray(img))\n","    plt.axis('off')\n","    plt.title(\"Input Image\", fontsize = 20)\n","    plt.show()\n","    img = transforms.ToTensor()(img).cuda()\n","    model = model_root().cuda().eval()\n","    model.load_state_dict(torch.load('../Models/svhn_root.pth'))\n","    print(\"Prediction: \", demo(model, img, torch.device(\"cuda\")))\n","\n","if __name__== \"__main__\":\n","        main()"],"metadata":{"id":"PmIGgAdSP1b5","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1673963740338,"user_tz":-480,"elapsed":2160,"user":{"displayName":"北科大-Ade Clinton","userId":"11192312192080613251"}},"outputId":"d111578f-7b83-48c3-8fda-b097f6a17ed9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Prediction:  3\n"]}]}]}